{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# This script creates propensity matched control group for the Lehman employees\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Main matching function for the given number of matches\n",
    "def matching(n_matches = 1,n_skills = 5):\n",
    "    # Go through the employment data, get people who worked at LEH and the control firms\n",
    "    employees = {}\n",
    "    tickers = {'LEH','GS','DB','UBS','MS^E'}\n",
    "    files = {'LEH_profiles.csv','GS_profiles.csv','DB_profiles.csv','UBS_profiles.csv','MS_profiles.csv'}\n",
    "    for item in files:\n",
    "        if item == 'LEH_profiles.csv':\n",
    "            lehman = 1\n",
    "        else:\n",
    "            lehman = 0\n",
    "        data = open('/Users/jqiu/Desktop/Fedyk/employment/lehman/replication/Data/' + item,'r')\n",
    "        emp_file = csv.reader(data,quoting=csv.QUOTE_MINIMAL,delimiter='\\t')\n",
    "        for columns in emp_file:\n",
    "            # get the dates of the job and make sure that the employment covered 1/1/2008\n",
    "            if columns[12]=='None':\n",
    "                from_dt = 19000101\n",
    "            else:\n",
    "                from_dt = int(columns[12][0:4])*10000+int(columns[12][5:7])*100+int(columns[12][8:10])\n",
    "            if columns[14]=='None':\n",
    "                to_dt = 20180101\n",
    "            else:\n",
    "                to_dt = int(columns[14][0:4])*10000+int(columns[14][5:7])*100+int(columns[14][8:10])\n",
    "            # write the employee to the appropriate firm dictionary, save the starting role as of 1/1/2008\n",
    "            if (columns[12] != 'None' or columns[14] != 'None') and from_dt < 20080101 and to_dt > 20080101 and columns[22] in tickers:\n",
    "                if columns[2]=='None':\n",
    "                    columns[2] = 1976\n",
    "                if columns[11]=='True':\n",
    "                    elite = 1\n",
    "                else:\n",
    "                    elite = 0\n",
    "                employees[columns[0]] = [lehman,'No role',columns[2],columns[3],columns[4],columns[6],columns[10],elite]\n",
    "    \n",
    "    print('Done gathering employee data')\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "#     # Cleanup uncommon roles\n",
    "#     threshold = 10\n",
    "#     roles = {}\n",
    "#     for item in employees:\n",
    "#         if employees[item][1] not in roles:\n",
    "#             roles[employees[item][1]] = 0\n",
    "#         roles[employees[item][1]] = roles[employees[item][1]]+1\n",
    "#     for item in employees:\n",
    "#         if roles[employees[item][1]]<threshold:\n",
    "#             employees[item][1] = 'Other'\n",
    "\n",
    "#     print('Done cleaning roles')\n",
    "#     print(datetime.datetime.now())\n",
    "    \n",
    "    # Convert to a dataframe\n",
    "    df = pd.DataFrame.from_dict(employees, orient='index',dtype=None)\n",
    "    df.columns = ['lehman','role','yob','gender','skill1','skill2','educ','elite']\n",
    "    cols_to_keep = ['lehman','elite','yob', 'educ', 'gender']\n",
    "    data = df[cols_to_keep]\n",
    "\n",
    "#     dummy_gender = pd.get_dummies(df['gender'],prefix = 'gender')\n",
    "#     gen = list(dummy_gender.columns.values)\n",
    "#     data = data.join(dummy_gender.ix[:,gen[1]:])\n",
    "#     dummy_educ = pd.get_dummies(df['educ'],prefix = 'educ')\n",
    "#     ed = list(dummy_educ.columns.values)\n",
    "#     data = data.join(dummy_educ.ix[:,ed[1]:])\n",
    "#     # Choose top 10 skills based on information gain\n",
    "#     dummy_skill1 = pd.get_dummies(df['skill1'],prefix = 'skill1')\n",
    "#     ig = information_gain(dummy_skill1,df['lehman'])\n",
    "#     s1 = list(dummy_skill1.columns.values)\n",
    "#     ind = np.array(ig[0]).argsort()[-n_skills:][::-1]\n",
    "#     for i in range(0,n_skills):\n",
    "#         data = data.join(dummy_skill1.ix[:,s1[ind[i]]])\n",
    "#     # Run the logistic regression matching on features\n",
    "#     train_cols = data.columns[1:]\n",
    "#     logit = sm.Logit(data['lehman'], data[train_cols].astype(float))\n",
    "#     result = logit.fit(maxiter = 5000)\n",
    "#     data['pred'] = result.predict(data[train_cols].astype(float))\n",
    "#     data['role'] = df['role']\n",
    "\n",
    "#     print ('Done with logistic regression matching on features')\n",
    "#     print (datetime.datetime.now())\n",
    "\n",
    "#     # For each Lehman person: first do exact match on role, then match on propensity scores of other features\n",
    "#     dummy_role = pd.get_dummies(df['role'],prefix = 'role')\n",
    "#     lehman = data[data['lehman']==1]\n",
    "#     nonlehman = data[data['lehman']==0]\n",
    "#     lehman_names = list(lehman.index)\n",
    "#     nonlehman_names = []\n",
    "#     count = 0\n",
    "#     for index, row in lehman.iterrows():\n",
    "#         sset = nonlehman[nonlehman['role']==row['role']]\n",
    "#         pred = abs(sset['pred']-row['pred'])\n",
    "#         ind = [i[0] for i in sorted(enumerate(pred), key=lambda x:x[1])]\n",
    "#         names = sset.iloc[ind[0:n_matches]].index.tolist()\n",
    "#         nonlehman_names = nonlehman_names + names\n",
    "#         count = count+1\n",
    "#         if count%1000==0:\n",
    "#             print(count)\n",
    "#             print(datetime.datetime.now())\n",
    "\n",
    "#     return [lehman_names, nonlehman_names, result]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS_profiles.csv\n",
      "MS_profiles.csv\n",
      "MS_profiles.csv\n",
      "MS_profiles.csv\n",
      "MS_profiles.csv\n",
      "MS_profiles.csv\n",
      "MS_profiles.csv\n",
      "MS_profiles.csv\n",
      "MS_profiles.csv\n",
      "LEH_profiles.csv\n",
      "LEH_profiles.csv\n",
      "LEH_profiles.csv\n",
      "LEH_profiles.csv\n",
      "LEH_profiles.csv\n",
      "LEH_profiles.csv\n",
      "LEH_profiles.csv\n",
      "LEH_profiles.csv\n",
      "LEH_profiles.csv\n"
     ]
    }
   ],
   "source": [
    "employees = {}\n",
    "tickers = {'LEH','GS','DB','UBS','MS^E'}\n",
    "files = ['GS_profiles.csv','DB_profiles.csv','UBS_profiles.csv','MS_profiles.csv', 'LEH_profiles.csv']\n",
    "for item in files:\n",
    "    if item == 'LEH_profiles.csv':\n",
    "        lehman = 1\n",
    "    else:\n",
    "        lehman = 0\n",
    "    data = open('/Users/jqiu/Desktop/Fedyk/employment/lehman/replication/Data/' + item,'r')\n",
    "    emp_file = csv.reader(data,quoting=csv.QUOTE_MINIMAL,delimiter='\\t')\n",
    "    for columns in emp_file:\n",
    "        # get the dates of the job and make sure that the employment covered 1/1/2008\n",
    "        #''company', 'normalized_company', 'ticker', 'industry', 'educational']\n",
    "        if columns[0] == 'c166dfbb-635e-350d-bb01-6dd831239ad8':\n",
    "            print(item)\n",
    "        if columns[12]=='None':\n",
    "            from_dt = 19000101\n",
    "        else:\n",
    "            from_dt = int(columns[12][0:4])*10000+int(columns[12][5:7])*100+int(columns[12][8:10])\n",
    "        if columns[14]=='None':\n",
    "            to_dt = 20180101\n",
    "        else:\n",
    "            to_dt = int(columns[14][0:4])*10000+int(columns[14][5:7])*100+int(columns[14][8:10])\n",
    "        # write the employee to the appropriate firm dictionary, save the starting role as of 1/1/2008\n",
    "        if (columns[12] != 'None' or columns[14] != 'None') and from_dt < 20080101 and to_dt > 20080101 and columns[22] in tickers:\n",
    "            if columns[2]=='None' or columns[2] == '2000':\n",
    "                columns[2] = 1976\n",
    "            if columns[11]=='True':\n",
    "                elite = 1\n",
    "            else:\n",
    "                elite = 0\n",
    "            employees[columns[0]] = [lehman,'No role',columns[2],columns[3],\\\n",
    "                                     columns[4],columns[6],columns[10],elite,\\\n",
    "                                     columns[20], columns[21],columns[22],columns[26], columns[27]]\n",
    "#     # Cleanup uncommon roles\n",
    "#     threshold = 10\n",
    "#     roles = {}\n",
    "#     for item in employees:\n",
    "#         if employees[item][1] not in roles:\n",
    "#             roles[employees[item][1]] = 0\n",
    "#         roles[employees[item][1]] = roles[employees[item][1]]+1\n",
    "#     for item in employees:\n",
    "#         if roles[employees[item][1]]<threshold:\n",
    "#             employees[item][1] = 'Other'\n",
    "\n",
    "#     print('Done cleaning roles')\n",
    "#     print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define supporting functions\n",
    "def _entropy(values):\n",
    "    counts = np.bincount(values)\n",
    "    probs = counts[np.nonzero(counts)] / float(len(values))\n",
    "    return - np.sum(probs * np.log2(probs))\n",
    "def information_gain(x, y):\n",
    "    feature_size = x.shape[0]\n",
    "    feature_range = range(0, feature_size)\n",
    "    entropy_before = _entropy(y)\n",
    "    information_gain_scores = []\n",
    "    for feature in x:\n",
    "        feature_set_indices = np.nonzero(x[feature])[0]\n",
    "        feature_not_set_indices = [i for i in feature_range if i not in feature_set_indices]\n",
    "        entropy_x_set = _entropy(y[feature_not_set_indices])\n",
    "        entropy_x_not_set = _entropy(y[feature_not_set_indices])\n",
    "        ent = entropy_before - (((len(feature_set_indices) / float(feature_size)) * entropy_x_set)\n",
    "                                 + ((len(feature_not_set_indices) / float(feature_size)) * entropy_x_not_set))\n",
    "        information_gain_scores.append(ent)\n",
    "    return information_gain_scores, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36709369026462935"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_entropy(my_all_data['is_lehman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9636"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data[all_data.is_lehman == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69316,  9628])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(all_data['is_lehman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/my_all_data', 'rb') as handle:\n",
    "    my_all_data = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe\n",
    "all_data = my_all_data.loc[:, ['user', 'is_lehman','role','birth','gender','primary','education','elite', 'company', 'normalized_company', 'ticker', 'industry', 'educational']]\n",
    "import pickle\n",
    "with open('./Data/employed_2016.csv', 'rb') as handle:\n",
    "    employed_2016 = pickle.load(handle)\n",
    "\n",
    "all_data.columns = ['user', 'is_lehman','role','birth','gender','primary','education','elite', 'company', 'normalized_company', 'ticker', 'industry', 'educational']\n",
    "all_data = all_data[all_data.user.isin(employed_2016)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = all_data[all_data.birth.isin(['None', '2000', None])].index\n",
    "all_data.loc[index, ['birth']] = '1976'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_data[['birth', 'gender', 'education', 'elite']].copy()\n",
    "X['education'] = X['education'].apply(str)\n",
    "X['gender'] = X['gender'].apply(str)\n",
    "X['birth'] = X['birth'].astype(int)\n",
    "X['elite'] = X['elite'].astype(int)\n",
    "\n",
    "X = pd.get_dummies(data=X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    71069\n",
       "1     9699\n",
       "Name: is_lehman, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['is_lehman'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'list' argument must have no negative elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-79fdb8300eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdummy_skill1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'primary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'primary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_skill1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_lehman'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-120-f634b46f3ce7>\u001b[0m in \u001b[0;36minformation_gain\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfeature_set_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mfeature_not_set_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_range\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_set_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mentropy_x_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_not_set_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mentropy_x_not_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_not_set_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         ent = entropy_before - (((len(feature_set_indices) / float(feature_size)) * entropy_x_set)\n",
      "\u001b[0;32m<ipython-input-120-f634b46f3ce7>\u001b[0m in \u001b[0;36m_entropy\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define supporting functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbincount\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'list' argument must have no negative elements"
     ]
    }
   ],
   "source": [
    "dummy_skill1 = pd.get_dummies(all_data['primary'],prefix = 'primary')\n",
    "ig = information_gain(dummy_skill1,all_data['is_lehman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = list(dummy_skill1.columns.values)\n",
    "ind = np.array(ig[0]).argsort()[-5:][::-1]\n",
    "for i in range(0,5):\n",
    "    X = X.join(dummy_skill1.ix[:,s1[ind[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.tools.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = all_data['is_lehman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = sm.Logit(y, X)\n",
    "result = logit.fit(maxiter = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
