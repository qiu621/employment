{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def main(csv_file):\n",
    "    \"\"\"\n",
    "    Given an appropriate csv_file, output the relevant columns.\n",
    "\n",
    "    Returns df with columns [user, start, end, normalized_company, industry]\n",
    "    \"\"\"\n",
    "\n",
    "    employ_data = pd.read_csv(csv_file, sep=\"\\t\", header=None,\n",
    "                              names=[i for i in range(34)], low_memory=False)\n",
    "    # column info from taxonomy file\n",
    "    name = ['user', 'name', 'birth', 'gender', 'primary',\n",
    "            'primary_weight', 'secondary', 'secondary_weight',\n",
    "            'city', 'country', 'education', 'elite', 'start',\n",
    "            '.', 'end', '??', '/', 'length', 'role', 'department',\n",
    "            'company', 'normalized_company', 'ticker', 'exchange',\n",
    "            'public', 'location_company', 'industry', 'educational',\n",
    "            'degree', 'elite_education', 'major', 'department', 'FIGI',\n",
    "            'last_update']\n",
    "#     drop = ['length', 'gender', 'primary',\n",
    "#         'primary_weight', 'secondary', 'secondary_weight',\n",
    "#         'city', 'country', 'education', 'elite', '.', '??',\n",
    "#         '/', 'department', 'exchange',\n",
    "#         'public', 'location_company', 'educational', 'degree', 'elite_education',\n",
    "#         'major', 'department', 'FIGI', 'last_update']\n",
    "    employ_data.columns = name\n",
    "    return employ_data\n",
    "\n",
    "def standardize_dates(company, missing_start = '1900-01-01', missing_end = '2018-01-01'):\n",
    "    \"\"\"\n",
    "    Converts start date and end date to datetime objects, and converts None values to the specified missing \n",
    "    dates.\n",
    "\n",
    "    Returns the modified dataframe\n",
    "    \"\"\"\n",
    "    company_data = raw_data[company].copy()\n",
    "    company_data['start'] = company_data['start'].str.replace('None', missing_start)\n",
    "    company_data['end'] = company_data['end'].str.replace('None', missing_end)\n",
    "    company_data['start'] = pd.to_datetime(company_data['start'])\n",
    "    company_data['end'] = pd.to_datetime(company_data['end'])\n",
    "    return company_data\n",
    "\n",
    "def get_users(company_name, company_data, worked_date = '2008-01-01', missing_start = '1900-01-01', missing_end = '2018-01-01'):\n",
    "    \"\"\"\"\n",
    "    Returns the users who worked at a given company on worked_date, that does not have both start and\n",
    "    end dates missing\n",
    "    \n",
    "    worked_date: string specifying the date on which to extract employees from. \n",
    "                 Must be coercible into a datetime object\n",
    "    missing_start: default value for missing start dates\n",
    "    missing_end: default value for missing end dates\n",
    "    \"\"\"\n",
    "    worked_date = pd.to_datetime(worked_date)\n",
    "    missing_start = pd.to_datetime(missing_start)\n",
    "    missing_end = pd.to_datetime(missing_end)\n",
    "    x = company_data\n",
    "    \n",
    "    company_tickers = {'db': 'DB', 'leh': 'LEH', 'gs': 'GS', 'ms': 'MS^E', 'ubs': 'UBS'}\n",
    "    # conditions: start and end not both missing, worked before/after 2008-01-01, ticker matches company\n",
    "    mask = ~((x['start'] == missing_start) & (x['end'] == missing_end)) & \\\n",
    "        (x['start'] < worked_date) & \\\n",
    "        (x['end'] > worked_date) & \\\n",
    "        (x['ticker'] == company_tickers[company_name])\n",
    "    return company_data[mask]['user'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# data without datetime features, and none values for some dates\n",
    "raw_data = {'db': main('./Data/DB_profiles.csv'),\n",
    "            'gs': main('./Data/GS_profiles.csv'),\n",
    "            'leh': main('./Data/LEH_profiles.csv'),\n",
    "            'ms': main('./Data/MS_profiles.csv'),\n",
    "            'ubs': main('./Data/UBS_profiles.csv')\n",
    "            }\n",
    "\n",
    "# add datetime features for each company\n",
    "for company in raw_data.keys():\n",
    "    raw_data[company] = standardize_dates(company)\n",
    "    \n",
    "# get the users that worked there as of 2008-01-01\n",
    "users = {company_name: get_users(company_name, company_data) for company_name, company_data in raw_data.items()}\n",
    "\n",
    "# filter entries to people that worked there as of 2008-01-01\n",
    "data = {}\n",
    "for company, company_data in raw_data.items():\n",
    "    company_users = users[company]\n",
    "    data[company] = company_data[company_data['user'].isin(company_users)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and categorizing job types as of 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def job_title_2008(company_name, company_data):\n",
    "    \"\"\"\"\n",
    "    Return each user's job title at the given company as of 2008-01-01\n",
    "    \"\"\"\n",
    "    date_2008 = pd.to_datetime('2008-01-01')\n",
    "    missing_start = pd.to_datetime('1900-01-01')\n",
    "    missing_end = pd.to_datetime('2018-01-01')\n",
    "\n",
    "    company_tickers = {'db': 'DB', 'leh': 'LEH', 'gs': 'GS', 'ms': 'MS^E', 'ubs': 'UBS'}\n",
    "\n",
    "    x = company_data\n",
    "    mask = ~((x['start'] == missing_start) & (x['end'] == missing_end)) & \\\n",
    "           (x['start'] < date_2008) & \\\n",
    "           (x['end'] > date_2008) & \\\n",
    "           (x['ticker'] == company_tickers[company_name])\n",
    "    return company_data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "drop = ['length', 'gender', 'primary',\n",
    "        'primary_weight', 'secondary', 'secondary_weight',\n",
    "        'city', 'country', 'education', 'elite', '.', '??',\n",
    "        '/', 'department', 'exchange',\n",
    "        'public', 'location_company', 'educational', 'degree', 'elite_education',\n",
    "        'major', 'department', 'FIGI', 'last_update', 'industry', 'birth', 'company']\n",
    "\n",
    "matching_data = {company_name: company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}\n",
    "\n",
    "\n",
    "\n",
    "job_title_as_of_2008 = {company_name: job_title_2008(company_name, company_data) for company_name, company_data in\n",
    "                  matching_data.items()}\n",
    "\n",
    "all_data = pd.concat(job_title_as_of_2008.values())\n",
    "# only person missing a role in the entire data set\n",
    "all_data = all_data.drop(11512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot index with vector containing NA / NaN values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-3c803932338e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_roles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0manalysts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'analyst|Anaylst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_roles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mall_roles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_roles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2969\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2970\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot index with vector containing NA / NaN values"
     ]
    }
   ],
   "source": [
    "# begin extracting job titles\n",
    "directors = set(all_data[(all_data.role.str.contains(r'director|MD,md', case=False))\n",
    "                         | (all_data.role.str.match(r'ed|md', case=False))].user)\n",
    "all_roles = directors.copy()\n",
    "\n",
    "analysts = set(all_data[all_data.role.str.contains('analyst|Anaylst', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(analysts)\n",
    "\n",
    "vps = set(all_data[all_data.role.str.contains('president|vp', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(vps)\n",
    "\n",
    "assocs = set(all_data[all_data.role.str.contains('associate', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(assocs)\n",
    "\n",
    "accountants = set(\n",
    "    all_data[all_data.role.str.contains('accountant|account executive|accounting', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(accountants)\n",
    "\n",
    "consultants = set(all_data[all_data.role.str.contains('consultant', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(consultants)\n",
    "\n",
    "missing = set(all_data[all_data.role.str.match(r'-|\\?|\\.', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(missing)\n",
    "\n",
    "developers = set(\n",
    "    all_data[all_data.role.str.contains(r'developer|engineer|system administrator', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(developers)\n",
    "\n",
    "interns = set(all_data[all_data.role.str.contains('intern|trainee|apprentice', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(interns)\n",
    "\n",
    "specialists = set(\n",
    "    all_data[all_data.role.str.contains('specialist|administrator|research|expert', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(specialists)\n",
    "\n",
    "sales = set(all_data[all_data.role.str.contains('sales', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(sales)\n",
    "\n",
    "traders = set(all_data[all_data.role.str.contains(r'trader|trading|Portfolio Management', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(traders)\n",
    "\n",
    "bankers = set(all_data[all_data.role.str.contains(r'banking|banker|finance', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(bankers)\n",
    "\n",
    "controllers = set(all_data[all_data.role.str.contains('controller', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(controllers)\n",
    "\n",
    "partners = set(all_data[all_data.role.str.contains('partner', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(partners)\n",
    "\n",
    "counsels = set(all_data[all_data.role.str.contains('counsel', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(counsels)\n",
    "\n",
    "recruiters = set(all_data[all_data.role.str.contains('recruiter|human resources', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(recruiters)\n",
    "\n",
    "advisors = set(all_data[all_data.role.str.contains('advisor|adviseur', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(advisors)\n",
    "\n",
    "assistants = set(\n",
    "    all_data[all_data.role.str.contains('assistant|support|services|receptionist', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(assistants)\n",
    "\n",
    "managers = set(all_data[all_data.role.str.contains(\n",
    "    r'manager|supervisor|team lead|head|lead|coordinator|representative|process executive',\n",
    "    case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(managers)\n",
    "\n",
    "others = set(all_data.user).difference(all_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# zip all sets and all job title names\n",
    "all_sets = [directors, analysts, vps, assocs, advisors, assistants, consultants, managers, missing, developers, interns,\n",
    "            specialists, sales, traders, bankers, controllers, partners, counsels, recruiters, accountants, others]\n",
    "job_titles = ['director', 'analyst', 'vp', 'assoc', 'advisor', 'assistant', 'consultant', 'manager', 'missing',\n",
    "              'developer', 'intern', 'specialist', 'sale', 'trader', 'banker', 'controller', 'partner', 'counsel',\n",
    "              'recruiter', 'accountant', 'other']\n",
    "\n",
    "zipped = list(zip(all_sets, job_titles))\n",
    "\n",
    "\n",
    "def to_dict(dictionary, users, job_title):\n",
    "    \"\"\"Map users to job_title in the given dictionary\"\"\"\n",
    "    for user in users:\n",
    "        dictionary.update({user: job_title})\n",
    "\n",
    "\n",
    "user_to_job_type = {}\n",
    "[to_dict(user_to_job_type, x, y) for x, y in zipped]\n",
    "user_to_job_type.update({'c0a3eb6a-59db-3a30-8a39-99a7cc8b9ce1': 'specialist'})\n",
    "user_to_job_type.update({'5f425323-1cdf-3e81-a08e-35b483c42da9': 'missing'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing industry mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# read all the csv files\n",
    "profile = pd.read_csv('./Data/profile_industry_mappings.csv', header=None, names=[i for i in range(5)], dtype={4: str})\n",
    "profile.drop([0, 2], axis='columns', inplace=True)\n",
    "profile.rename(mapper={1: 'company', 3: 'norm', 4: \"ind\"}, axis='columns', inplace=True)\n",
    "\n",
    "mturk = pd.read_csv('./Data/industries_MTurkers_20170711.csv', header=None, encoding='latin-1')\n",
    "mturk.drop([1], axis='columns', inplace=True)\n",
    "mturk.rename(mapper={0: 'company', 2: \"ind\"}, axis='columns', inplace=True)\n",
    "\n",
    "finance = pd.read_csv('./Data/Finance.csv', dtype={'Industry': str})\n",
    "finance.drop([finance.columns[0], finance.columns[2], finance.columns[4]], axis='columns', inplace=True)\n",
    "finance.rename(mapper={'Normalized Company Name': 'norm', 'Industry': \"ind\"}, axis='columns', inplace=True)\n",
    "\n",
    "manual = pd.read_csv('./Data/manual_industry_mappings.csv', encoding='latin-1', header=None, dtype={2: str})\n",
    "manual.drop([1], axis='columns', inplace=True)\n",
    "manual.rename(mapper={0: 'norm', 2: \"ind\"}, axis='columns', inplace=True)\n",
    "\n",
    "industries_2019 = pd.read_csv('./Data/missing_industries_2019.csv', header=None, dtype={2: str})\n",
    "industries_2019 = industries_2019[~(industries_2019[1] == 1)].copy()\n",
    "\n",
    "industries_2019.drop([1], axis = 'columns', inplace = True)\n",
    "industries_2019.rename(mapper={0: 'company', 2: \"ind\"}, axis='columns', inplace=True)\n",
    "industries_2019 = industries_2019[~pd.isnull(industries_2019.ind)].copy()\n",
    "\n",
    "#mturk industry is given as \"ind_x\", profile industry is given as \"ind_y\"\n",
    "company_comb = pd.merge(mturk, profile, on='company', how='outer')\n",
    "#prioritize mturk data\n",
    "company_comb['combined'] = company_comb['ind_x'].combine_first(company_comb['ind_y'])\n",
    "\n",
    "#mturk industry is given as \"ind\", profile industry is given as \"combined\"\n",
    "company_comb = pd.merge(industries_2019, company_comb, on='company', how='outer')\n",
    "#prioritize manual entry data\n",
    "company_comb['combined'] = company_comb['ind'].combine_first(company_comb['combined'])\n",
    "\n",
    "#merge manual and finance files, prioritizing manual\n",
    "norm_comb = pd.merge(manual, finance, on = 'norm', how = 'outer')\n",
    "norm_comb['combined'] = norm_comb['ind_x'].combine_first(norm_comb['ind_y'])\n",
    "#merge manual/finance and profile[norm], prioritizing manual/finance\n",
    "norm_comb = pd.merge(norm_comb, profile, on = 'norm', how = 'outer')\n",
    "norm_comb['combined'] = norm_comb['combined'].combine_first(norm_comb['ind'])\n",
    "\n",
    "# convert the columns of the aggredated dataframe into a dictionary where the key is the company name\n",
    "# and the value is the industry code\n",
    "norm_mapping = dict(zip(norm_comb.norm, norm_comb.combined))\n",
    "company_mapping = dict(zip(company_comb.company, company_comb.combined))\n",
    "# set the default value if the company is not found to NaN\n",
    "norm_mapping = defaultdict(lambda: np.NaN, norm_mapping)\n",
    "company_mapping = defaultdict(lambda: np.NaN, company_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def add_industry_labels(company_data):\n",
    "    company_data = company_data.copy()\n",
    "    # convert to lowercase for more accurate matching\n",
    "    company_data['normalized_company_lower'] = company_data['normalized_company'].str.lower()\n",
    "    company_data['company_lower'] = company_data['company'].str.lower()\n",
    "    # apply norm_mapping and company_mapping to upper and lower case versions\n",
    "    company_data['company_mapped'] = company_data['company'].apply(lambda y: company_mapping[y])\n",
    "    company_data['normalized_company_mapped'] = company_data['normalized_company'].apply(lambda y: norm_mapping[y])\n",
    "    company_data['company_lower_mapped'] = company_data['normalized_company_lower'].apply(lambda y: norm_mapping[y])\n",
    "    company_data['normalized_company_lower_mapped'] = company_data['normalized_company_lower'].apply(lambda y: norm_mapping[y])\n",
    "    # combines all mappings. Prioritize Existing industry code > MTurk/profle(company) > \n",
    "    # manual/finance/profile(normalized_company) > manual/finance/profile(normalized_company_lower) \n",
    "    company_data['industry_two'] = company_data['industry'].combine_first(company_data['company_mapped'])\n",
    "    company_data['industry_three'] = company_data['industry_two'].combine_first(company_data['normalized_company_mapped'])\n",
    "    company_data['industry_four'] = company_data['industry_three'].combine_first(company_data['company_lower_mapped'])\n",
    "    company_data['industry_five'] = company_data['industry_four'].combine_first(company_data['normalized_company_lower_mapped'])\n",
    "    company_data['industry'] = company_data['industry_five']\n",
    "    # drop the temporary columns\n",
    "    company_data.drop(['normalized_company_lower', 'company_lower', 'company_mapped', 'normalized_company_mapped', 'company_lower_mapped','normalized_company_lower_mapped', 'industry_two', 'industry_three', 'industry_four','industry_five'], axis=1, inplace=True)\n",
    "    return company_data\n",
    "\n",
    "def before_2016(company_data):\n",
    "    \"\"\"\n",
    "    Return entries before 2016-1-1, excluding those with no start or end time, or are educational.\n",
    "    \"\"\"\n",
    "    mask = (company_data['start'] <= pd.to_datetime('2016-1-1')) & (company_data['end'] >= pd.to_datetime('2016-1-1')) & ~((company_data['start'] == pd.to_datetime('1900-01-01')) & (company_data['end'] == pd.to_datetime('2018-01-01'))) & (~company_data['ticker'].isin(['UNIVERSITY', 'SCHOOL']) & ~(company_data.educational) & ~(company_data['industry'].isin(['UNIVERSITY', 'SCHOOL'])))\n",
    "    return company_data[mask]\n",
    "\n",
    "\n",
    "def labeled_2016(company_data):\n",
    "    # combines filter and mask\n",
    "    labeled = add_industry_labels(company_data)\n",
    "    return before_2016(labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most informative skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "def n_most_informative_skills(n, all_data):\n",
    "    skills = list(all_data.primary.unique())\n",
    "    information_gains = []\n",
    "    num_people = len(all_data)\n",
    "    num_lehman = sum(all_data.is_lehman)\n",
    "    num_non_lehman = num_people - num_lehman\n",
    "    entropy_parent = st.entropy([num_lehman, num_non_lehman],base=2)\n",
    "    \n",
    "    for skill in skills:\n",
    "        with_skill = all_data[all_data.primary == skill]\n",
    "        without_skill = all_data[~(all_data.primary == skill)]\n",
    "        num_with_skill = len(with_skill)\n",
    "        num_without_skill = len(without_skill)\n",
    "        \n",
    "        with_skill_lehman = sum(with_skill.is_lehman)\n",
    "        with_skill_non_lehman = num_with_skill - with_skill_lehman\n",
    "        entropy_with_skill = st.entropy([with_skill_lehman, with_skill_non_lehman],base=2)\n",
    "        \n",
    "        without_skill_lehman = sum(without_skill.is_lehman)\n",
    "        without_skill_non_lehman = num_without_skill - without_skill_lehman\n",
    "        entropy_without_skill = st.entropy([without_skill_lehman, without_skill_non_lehman],base=2)\n",
    "        \n",
    "        conditional_entropy = (num_with_skill/num_people * entropy_with_skill \n",
    "                               + num_without_skill/num_people * entropy_without_skill)\n",
    "        \n",
    "        information_gain_split = entropy_parent - conditional_entropy\n",
    "        information_gains.append(information_gain_split)\n",
    "        \n",
    "    sorted_ig_indices = np.flip(np.argsort(information_gains))\n",
    "    skills_by_ig = [skills[index] for index in sorted_ig_indices]\n",
    "    return skills_by_ig[0:n]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.discrete.discrete_model as sm\n",
    "# prepare data for regression by dropping irrelevant names\n",
    "drop = ['length', 'name',\n",
    "        'primary_weight', 'secondary', 'secondary_weight', 'elite_education',\n",
    "        'city', 'country', '.', '??',\n",
    "        '/', 'department', 'exchange',\n",
    "        'public', 'location_company',\n",
    "        'major', 'department', 'FIGI', 'last_update', 'degree']\n",
    "\n",
    "regression_data = {company_name: company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}\n",
    "\n",
    "#additional step of labeling missing industries and only look at those with jobs as of 2016-1-1\n",
    "all_data_2016 = pd.concat(regression_data.values())\n",
    "employed_2016 = labeled_2016(all_data_2016)\n",
    "employ_2016_users = list(employed_2016.user.unique())\n",
    "\n",
    "regression_data = {company_name: job_title_2008(company_name, company_data) for company_name, company_data in\n",
    "                   regression_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "non_lehman = pd.concat([regression_data['db'], regression_data['gs'], regression_data['ms'], regression_data['ubs']])\n",
    "non_lehman['is_lehman'] = 0\n",
    "\n",
    "lehman = regression_data['leh'].copy()\n",
    "lehman['is_lehman'] = 1\n",
    "\n",
    "all_data = pd.concat([lehman, non_lehman])\n",
    "all_data = all_data[all_data.user.isin(employ_2016_users)]\n",
    "\n",
    "# fill in missing births to the median date, 1976\n",
    "index = all_data[all_data.birth.isin(['None', '2000'])].index\n",
    "all_data.loc[index, ['birth']] = '1976'\n",
    "FINAL_REGRESSION_DATA = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_matches(num_matches, num_skills):\n",
    "    all_data = FINAL_REGRESSION_DATA.copy()\n",
    "    informative_skills = n_most_informative_skills(num_skills, all_data)\n",
    "    not_informative = ~all_data.primary.isin(informative_skills)\n",
    "    all_data.loc[not_informative, 'primary'] = 0\n",
    "    all_data['job_category'] = all_data.user.apply(lambda x: user_to_job_type[x])\n",
    "\n",
    "    X = all_data[['birth', 'gender', 'primary', 'education', 'elite']].copy()\n",
    "    X['education'] = X['education'].apply(str)\n",
    "    X['gender'] = X['gender'].apply(str)\n",
    "    X['birth'] = X['birth'].astype(int)\n",
    "    X['elite'] = X['elite'].astype(int)\n",
    "    X = pd.get_dummies(data=X, drop_first=True)\n",
    "    X = sm.tools.add_constant(X)\n",
    "\n",
    "    y = all_data['is_lehman']\n",
    "\n",
    "    # regress y on X\n",
    "    logit = sm.Logit(y, X)\n",
    "    results = logit.fit(maxiter = 100)\n",
    "\n",
    "    # get propensities\n",
    "    all_data['propensity'] = results.predict(X)\n",
    "\n",
    "    # Begin matching process. Map each user to its propensity\n",
    "    user_to_propensity = dict(zip(all_data.user, all_data.propensity))\n",
    "\n",
    "    # get lehman and non-lehman guys\n",
    "    lehman = all_data[all_data['is_lehman'] == 1]\n",
    "    non_lehman = all_data[all_data['is_lehman'] == 0]\n",
    "    \n",
    "    def flatten(list_of_lists):\n",
    "        return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "    def get_closest(n, row):\n",
    "        # return n nearest neighbors of closest match containing the same job title\n",
    "        role = row.job_category\n",
    "        score = row.propensity\n",
    "        others_by_role = non_lehman[non_lehman.job_category == role].reset_index().loc[:, ['user', 'propensity']]\n",
    "        absolute_propensity = np.absolute(others_by_role['propensity'] - score)\n",
    "        ind = np.argsort(absolute_propensity)\n",
    "        matches = others_by_role.loc[ind[0:n]].user.to_list()\n",
    "        return matches\n",
    "\n",
    "\n",
    "    # get closest match for each lehman guy\n",
    "    lehman['matches'] = lehman.apply(lambda x : get_closest(num_matches, x), axis=1)\n",
    "\n",
    "    return lehman.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matched stayed in finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.363000\n",
      "         Iterations 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.363000\n",
      "         Iterations 8\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.363000\n",
      "         Iterations 8\n"
     ]
    }
   ],
   "source": [
    "one = return_matches(num_matches = 1, num_skills = 5)\n",
    "two = return_matches(num_matches = 2, num_skills = 5)\n",
    "five = return_matches(num_matches = 5, num_skills = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_match_stayed(num_matches = 5, num_skills = 5):    \n",
    "    matched_data = return_matches(num_matches, num_skills)\n",
    "    unique_matches = set(flatten(list(matched_data.matches)))\n",
    "    drop = ['length', 'gender', 'primary',\n",
    "            'primary_weight', 'secondary', 'secondary_weight',\n",
    "            'city', 'country', 'education', 'elite', '.', '??',\n",
    "            '/', 'department', 'exchange',\n",
    "            'public', 'location_company', 'degree', 'elite_education',\n",
    "            'major', 'department', 'FIGI', 'last_update']\n",
    "\n",
    "    finance_data = {company_name : company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}\n",
    "\n",
    "    labeled_2016_job_data = {company_name: labeled_2016(company_data) for company_name, company_data in finance_data.items()}\n",
    "\n",
    "    lehman_to_matches = dict(zip(matched_data.user, matched_data.matches))\n",
    "\n",
    "    lehman = labeled_2016_job_data['leh'].copy()\n",
    "    lehman['matches'] = lehman.user.apply(lambda x : lehman_to_matches[x])\n",
    "\n",
    "\n",
    "    non_lehman =  pd.concat([labeled_2016_job_data['db'], \n",
    "                             labeled_2016_job_data['gs'], \n",
    "                             labeled_2016_job_data['ms'], \n",
    "                             labeled_2016_job_data['ubs']])\n",
    "    matches = non_lehman[non_lehman.user.isin(unique_matches)]\n",
    "\n",
    "    matches = matches.groupby('user').first().reset_index()\n",
    "    matches['stayed_finance'] = matches['industry'].str.startswith('52', na=False)\n",
    "\n",
    "    match_to_stayed_finance = dict(zip(matches.user, matches.stayed_finance))\n",
    "\n",
    "    lehman_2016_job_data = lehman.copy()\n",
    "    lehman_2016_job_data = lehman_2016_job_data.groupby('user').first()\n",
    "    lehman_2016_job_data = lehman_2016_job_data.explode('matches')\n",
    "\n",
    "    lehman_2016_job_data['match_stayed_finance'] = lehman_2016_job_data.matches.apply(lambda match: match_to_stayed_finance[match])\n",
    "    lehman_2016_job_data['lehman_stayed_finance'] = lehman_2016_job_data.industry.str.startswith('52', na = False)\n",
    "    lehman_2016_job_data['job_category'] = lehman_2016_job_data.index.to_series().apply(lambda x: user_to_job_type[x])\n",
    "    return lehman_2016_job_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "yOne = return_match_stayed(one)\n",
    "yTwo = return_match_stayed(two)\n",
    "yFive = return_match_stayed(five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probit propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_match = calculate_propensity_SE_by_roles(yOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_matches = calculate_propensity_SE_by_roles(yTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_matches = calculate_propensity_SE_by_roles(yFive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{lrrrrr}\\n\\\\toprule\\n{} &  full sample &        vp &   analyst &  director &     assoc \\\\\\\\\\n\\\\midrule\\n0 &     0.059643 &  0.117491 &  0.031251 &  0.106497 &  0.102683 \\\\\\\\\\n1 &     0.002249 &  0.004079 &  0.005242 &  0.007279 &  0.007929 \\\\\\\\\\n0 &     0.059643 &  0.117491 &  0.031251 &  0.106497 &  0.102683 \\\\\\\\\\n1 &     0.002249 &  0.004079 &  0.005242 &  0.007279 &  0.007929 \\\\\\\\\\n0 &     0.059643 &  0.117491 &  0.031251 &  0.106497 &  0.102683 \\\\\\\\\\n1 &     0.002249 &  0.004079 &  0.005242 &  0.007279 &  0.007929 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([one_match, two_matches, five_matches]).to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full sample</th>\n",
       "      <th>vp</th>\n",
       "      <th>analyst</th>\n",
       "      <th>director</th>\n",
       "      <th>assoc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.059643</td>\n",
       "      <td>0.117491</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.106497</td>\n",
       "      <td>0.102683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.004079</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.007279</td>\n",
       "      <td>0.007929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   full sample        vp   analyst  director     assoc\n",
       "0     0.059643  0.117491  0.031251  0.106497  0.102683\n",
       "1     0.002249  0.004079  0.005242  0.007279  0.007929"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_propensity_SE_by_roles(data, roles = ['full sample', 'vp', 'analyst', 'director', 'assoc']):\n",
    "    dataDict = {}\n",
    "    for role in roles:\n",
    "        dataDict[role] = calculate_propensity_SE(role, data)\n",
    "    return pd.DataFrame.from_dict(dataDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_propensity_SE(job_category, lehman_2016_job_data):\n",
    "    \"\"\"names: [full sample, vp, analyst, director, assoc]\"\"\"\n",
    "    lehman_guys = lehman_2016_job_data[['lehman_stayed_finance', 'job_category']].copy().rename({'lehman_stayed_finance' : 'stayed_finance', 'job_category' : 'job'}, axis = 1)\n",
    "    lehman_guys['is_lehman'] = 1\n",
    "    non_lehman_guys = lehman_2016_job_data[['match_stayed_finance', 'job_category']].copy().rename({'match_stayed_finance' : 'stayed_finance', 'job_category' : 'job'}, axis = 1)\n",
    "    non_lehman_guys['is_lehman'] = 0\n",
    "\n",
    "    data_by_job = pd.concat([lehman_guys,non_lehman_guys])\n",
    "    if job_category != 'full sample':\n",
    "        data_by_job = data_by_job[data_by_job.job == job_category]\n",
    "    \n",
    "    assert len(data_by_job != 0 ), 'job category not valid'\n",
    "    X = data_by_job['is_lehman']\n",
    "    y = data_by_job['stayed_finance']\n",
    "    \n",
    "    model = sm.Probit(y, X)\n",
    "    results = model.fit(disp = 0)\n",
    "    summary = results.get_margeff().summary_frame().iloc[:, 0:2]\n",
    "    dydx = summary.iloc[0,0]\n",
    "    SE = summary.iloc[0,1]\n",
    "    return (dydx, SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst associate vp MD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = lehman_2016_job_data.job_category.unique()\n",
    "stayed_finance = {'role' : [], 'lehman_stayed' : [], 'match_stayed' : [], 'total' : [], 'proportion_lehman':[], 'proportion_match' :[], 'zscore': []}\n",
    "\n",
    "for role in roles:\n",
    "    role_data = lehman_2016_job_data[lehman_2016_job_data.job_category == role]\n",
    "    lehman_stayed = sum(role_data.industry.str.startswith('52', na = False))\n",
    "    match_stayed = sum(role_data.match_stayed_finance)\n",
    "    total = len(role_data)\n",
    "    prop_lehman = lehman_stayed/total\n",
    "    prop_match = match_stayed/total\n",
    "    zscore = (prop_lehman - prop_match) / (prop_lehman * prop_match *(2/total))**(1/2)\n",
    "    stayed_finance['role'].append(role)\n",
    "    stayed_finance['lehman_stayed'].append(lehman_stayed)\n",
    "    stayed_finance['match_stayed'].append(match_stayed)\n",
    "    stayed_finance['total'].append(total)\n",
    "    stayed_finance['proportion_lehman'].append(prop_lehman)\n",
    "    stayed_finance['proportion_match'].append(prop_match)\n",
    "    stayed_finance['zscore'].append(zscore)\n",
    "role_data = lehman_2016_job_data[lehman_2016_job_data.job_category == role]\n",
    "\n",
    "lehman_stayed = sum(lehman_2016_job_data.industry.str.startswith('52', na = False))\n",
    "match_stayed = sum(lehman_2016_job_data.match_stayed_finance)\n",
    "total = len(lehman_2016_job_data)\n",
    "prop_lehman = lehman_stayed/total\n",
    "prop_match = match_stayed/total\n",
    "zscore = (prop_lehman - prop_match) / (prop_lehman * prop_match *(2/total))**(1/2)\n",
    "\n",
    "stayed_finance['role'].append('all_roles')\n",
    "stayed_finance['lehman_stayed'].append(lehman_stayed)\n",
    "stayed_finance['match_stayed'].append(match_stayed)\n",
    "stayed_finance['total'].append(total)\n",
    "stayed_finance['proportion_lehman'].append(lehman_stayed/total)\n",
    "stayed_finance['proportion_match'].append(match_stayed/total)\n",
    "stayed_finance['zscore'].append(zscore)\n",
    "\n",
    "df = pd.DataFrame(stayed_finance)\n",
    "\n",
    "df = df[df.total>= 200]\n",
    "\n",
    "toPlot = df[['role', 'proportion_lehman', 'proportion_match']].set_index('role').stack().reset_index()\n",
    "toPlot = toPlot.rename({'level_1' : 'company', 0 : 'proportion'}, axis = 1)\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,5)})\n",
    "\n",
    "sns.lineplot( x = 'role', y = 'proportion', hue = 'company', data = toPlot);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
