{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def main(csv_file):\n",
    "    \"\"\"\n",
    "    Given an appropriate csv_file, output the relevant columns.\n",
    "\n",
    "    Returns df with columns [user, start, end, normalized_company, industry]\n",
    "    \"\"\"\n",
    "\n",
    "    employ_data = pd.read_csv(csv_file, sep=\"\\t\", header=None,\n",
    "                              names=[i for i in range(34)], low_memory=False)\n",
    "    # column info from taxonomy file\n",
    "    name = ['user', 'name', 'birth', 'gender', 'primary',\n",
    "            'primary_weight', 'secondary', 'secondary_weight',\n",
    "            'city', 'country', 'education', 'elite', 'start',\n",
    "            '.', 'end', '??', '/', 'length', 'role', 'department',\n",
    "            'company', 'normalized_company', 'ticker', 'exchange',\n",
    "            'public', 'location_company', 'industry', 'educational',\n",
    "            'degree', 'elite_education', 'major', 'department', 'FIGI',\n",
    "            'last_update']\n",
    "#     drop = ['length', 'gender', 'primary',\n",
    "#         'primary_weight', 'secondary', 'secondary_weight',\n",
    "#         'city', 'country', 'education', 'elite', '.', '??',\n",
    "#         '/', 'department', 'exchange',\n",
    "#         'public', 'location_company', 'educational', 'degree', 'elite_education',\n",
    "#         'major', 'department', 'FIGI', 'last_update']\n",
    "    employ_data.columns = name\n",
    "    return employ_data\n",
    "\n",
    "\n",
    "# data without datetime features, and none values for some dates\n",
    "raw_data = {'db': main('./Data/DB_profiles.csv'),\n",
    "            'gs': main('./Data/GS_profiles.csv'),\n",
    "            'leh': main('./Data/LEH_profiles.csv'),\n",
    "            'ms': main('./Data/MS_profiles.csv'),\n",
    "            'ubs': main('./Data/UBS_profiles.csv')\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def standardize_dates(company):\n",
    "    \"\"\"\n",
    "    Converts start date and end date to datetime objects, and converts None values to the minimum or maximum date.\n",
    "\n",
    "    Returns the modified dataframe\n",
    "    \"\"\"\n",
    "    company_data = raw_data[company].copy()\n",
    "    company_data['start'] = company_data['start'].str.replace('None', '1900-01-01')\n",
    "    company_data['end'] = company_data['end'].str.replace('None', '2018-01-01')\n",
    "    company_data['start'] = pd.to_datetime(company_data['start'])\n",
    "    company_data['end'] = pd.to_datetime(company_data['end'])\n",
    "    return company_data\n",
    "\n",
    "\n",
    "# set up dictionary to hold data for each company\n",
    "\n",
    "for company in raw_data.keys():\n",
    "    raw_data[company] = standardize_dates(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_users(company_name, company_data):\n",
    "    \"\"\"\"\n",
    "    Returns the users who worked at the given company before and and after 2008-01-01, exclusive\n",
    "    \"\"\"\n",
    "    date_2008 = pd.to_datetime('2008-01-01')\n",
    "    missing_start = pd.to_datetime('1900-01-01')\n",
    "    missing_end = pd.to_datetime('2018-01-01')\n",
    "    \n",
    "    x = company_data\n",
    "    \n",
    "    company_tickers = {'db': 'DB', 'leh': 'LEH', 'gs': 'GS', 'ms': 'MS^E', 'ubs': 'UBS'}\n",
    "    # conditions: start and end not both missing, worked before/after 2008-01-01, ticker matches company\n",
    "    mask = ~((x['start'] == missing_start) & (x['end'] == missing_end)) & \\\n",
    "        (x['start'] < date_2008) & \\\n",
    "        (x['end'] > date_2008) & \\\n",
    "        (x['ticker'] == company_tickers[company_name])\n",
    "    return company_data[mask]['user'].unique()\n",
    "\n",
    "\n",
    "# gets the user_ids within each company that match the conditioning, before and and after\n",
    "#   2008-01-01, exclusive\n",
    "users = {company_name: get_users(company_name, company_data) for company_name, company_data in raw_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for company, company_data in raw_data.items():\n",
    "    company_users = users[company]\n",
    "    data[company] = company_data[company_data['user'].isin(company_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAKS#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "drop = ['length', 'gender', 'primary',\n",
    "        'primary_weight', 'secondary', 'secondary_weight',\n",
    "        'city', 'country', 'education', 'elite', '.', '??',\n",
    "        '/', 'department', 'exchange',\n",
    "        'public', 'location_company', 'educational', 'degree', 'elite_education',\n",
    "        'major', 'department', 'FIGI', 'last_update', 'name', 'birth','end', 'role',\n",
    "        'company','normalized_company','industry']\n",
    "\n",
    "breaks_data = {company_name : company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prop_breaks(company_name, company_data):\n",
    "    \"\"\"\n",
    "    Outputs proportion of \"TIME_OFF\" or \"MISSING\" entries in a\n",
    "    dataset as the tuple (numerator, denominator).\n",
    "    \"\"\"\n",
    "\n",
    "    def get_breaks():\n",
    "        \"\"\"\n",
    "        Outputs the number of employees that have any \"TIME_OFF\" or \"MISSING\" entries after 2008.\n",
    "        \"\"\"\n",
    "        date_2008 = pd.to_datetime('2008-01-01')\n",
    "        # look only at data after 2008\n",
    "        after_2008 = company_data[company_data['start'] > date_2008]\n",
    "        # groupby user, aggregate by looking at the ticker and seeing if the person has had any time off\n",
    "        return sum(after_2008.groupby('user').ticker.agg(lambda x: any((x == 'TIME_OFF') | (x == 'MISSING'))))\n",
    "    \n",
    "    num_company_users = len(users[company_name])\n",
    "    num_breaks = get_breaks()\n",
    "    return company_name, num_breaks, num_company_users\n",
    "\n",
    "\n",
    "prop_breaks_company = [prop_breaks(company_name, company_data) for company_name, company_data in breaks_data.items()]\n",
    "# final proportions by company in dict{company_name : proportion_breaks} form\n",
    "prop_breaks_company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATCHING BY ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "drop = ['length', 'gender', 'primary',\n",
    "            'primary_weight', 'secondary', 'secondary_weight',\n",
    "            'city', 'country', 'education', 'elite', '.', '??',\n",
    "            '/', 'department', 'exchange',\n",
    "            'public', 'location_company', 'educational', 'degree', 'elite_education',\n",
    "            'major', 'department', 'FIGI', 'last_update', 'industry','birth','company']\n",
    "matching_data = {company_name : company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def job_2008(company_name, company_data):\n",
    "    \"\"\"\"\n",
    "    Returns the entry containing each user's job as of 2008-01-01\n",
    "    \"\"\"\n",
    "    date_2008 = pd.to_datetime('2008-01-01')\n",
    "    missing_start = pd.to_datetime('1900-01-01')\n",
    "    missing_end = pd.to_datetime('2018-01-01')\n",
    "    \n",
    "    company_tickers = {'db': 'DB', 'leh': 'LEH', 'gs': 'GS', 'ms': 'MS^E', 'ubs': 'UBS'}\n",
    "    \n",
    "    x = company_data\n",
    "    mask = ~((x['start'] == missing_start) & (x['end'] == missing_end)) & \\\n",
    "        (x['start'] < date_2008) & \\\n",
    "        (x['end'] > date_2008) & \\\n",
    "        (x['ticker'] == company_tickers[company_name])\n",
    "    return company_data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "job_as_of_2008 = {company_name: job_2008(company_name, company_data) for company_name, company_data in matching_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat(job_as_of_2008.values())\n",
    "#only person missing a role in the entire data set\n",
    "all_data = all_data.drop(11512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "directors = set(all_data[(all_data.role.str.contains(r'director|MD,md', case = False)) \n",
    "                         | (all_data.role.str.match(r'ed|md', case = False))].user)\n",
    "all_roles = directors.copy()\n",
    "\n",
    "analysts = set(all_data[all_data.role.str.contains('analyst|Anaylst', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(analysts)\n",
    "\n",
    "vps = set(all_data[all_data.role.str.contains('president|vp', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(vps)\n",
    "\n",
    "assocs = set(all_data[all_data.role.str.contains('associate', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(assocs)\n",
    "\n",
    "accountants = set(all_data[all_data.role.str.contains('accountant|account executive|accounting',case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(accountants)\n",
    "\n",
    "consultants = set(all_data[all_data.role.str.contains('consultant', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(consultants)\n",
    "\n",
    "missing = set(all_data[all_data.role.str.match(r'-|\\?|\\.', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(missing)\n",
    "\n",
    "developers = set(all_data[all_data.role.str.contains(r'developer|engineer|system administrator', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(developers)\n",
    "\n",
    "interns = set(all_data[all_data.role.str.contains('intern|trainee|apprentice', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(interns)\n",
    "\n",
    "specialists = set(all_data[all_data.role.str.contains('specialist|administrator|research|expert', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(specialists)\n",
    "\n",
    "sales = set(all_data[all_data.role.str.contains('sales', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(sales)\n",
    "\n",
    "traders = set(all_data[all_data.role.str.contains(r'trader|trading|Portfolio Management', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(traders)\n",
    "\n",
    "bankers = set(all_data[all_data.role.str.contains(r'banking|banker|finance', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(bankers)\n",
    "\n",
    "controllers = set(all_data[all_data.role.str.contains('controller', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(controllers)\n",
    "\n",
    "partners = set(all_data[all_data.role.str.contains('partner', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(partners)\n",
    "\n",
    "counsels = set(all_data[all_data.role.str.contains('counsel', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(counsels)\n",
    "\n",
    "recruiters = set(all_data[all_data.role.str.contains('recruiter|human resources', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(recruiters)\n",
    "\n",
    "advisors = set(all_data[all_data.role.str.contains('advisor|adviseur', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(advisors)\n",
    "\n",
    "assistants = set(all_data[all_data.role.str.contains('assistant|support|services|receptionist', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(assistants)\n",
    "\n",
    "managers = set(all_data[all_data.role.str.contains(r'manager|supervisor|team lead|head|lead|coordinator|representative|process executive', case = False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(managers)\n",
    "\n",
    "others = set(all_data.user).difference(all_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# remaining = all_data[~all_data.user.isin(all_roles)].copy()\n",
    "\n",
    "# remaining.role.value_counts().to_csv('./Deliverables/uncategorized_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "all_sets = [directors, analysts, vps, assocs, advisors, assistants, consultants, managers, missing, developers, interns, specialists, sales, traders, bankers, controllers, partners, counsels, recruiters, accountants, others]\n",
    "job_titles = ['director', 'analyst','vp', 'assoc','advisor','assistant','consultant','manager','missing','developer', 'intern', 'specialist','sale','trader','banker','controller','parnter','counsel', 'recruiter','accountant','other']\n",
    "\n",
    "zipped = list(zip(all_sets,job_titles))\n",
    "\n",
    "def to_dict(dictionary, users, job_title):\n",
    "    for user in users:\n",
    "        dictionary.update({user:job_title})\n",
    "\n",
    "full_mapping = {}\n",
    "[to_dict(full_mapping, x, y) for x,y in zipped]\n",
    "full_mapping.update({'c0a3eb6a-59db-3a30-8a39-99a7cc8b9ce1' : 'specialist'})\n",
    "full_mapping.update({'5f425323-1cdf-3e81-a08e-35b483c42da9' : 'missing'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "all_data['job_category'] = all_data.user.apply(lambda x: full_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# all_data.to_csv('./Deliverables/all_data_categorized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "drop = ['length','name','industry',\n",
    "            'primary_weight', 'secondary', 'secondary_weight', 'elite_education',\n",
    "            'city', 'country', '.', '??',\n",
    "            '/', 'department', 'exchange',\n",
    "            'public', 'location_company',\n",
    "            'major', 'department', 'FIGI', 'last_update','company','normalized_company','educational','degree']\n",
    "    \n",
    "regression_data = {company_name : company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}\n",
    "\n",
    "regression_data = {company_name: job_2008(company_name, company_data) for company_name, company_data in regression_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "non_lehman =  pd.concat([regression_data['db'], regression_data['gs'], regression_data['ms'], regression_data['ubs']])\n",
    "non_lehman['is_lehman'] = 0\n",
    "\n",
    "lehman = regression_data['leh'].copy()\n",
    "lehman['is_lehman'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# def condition(company, dataset):\n",
    "#     company_tickers = {'db': 'DB', 'leh': 'LEH', 'gs': 'GS', 'ms': 'MS^E', 'ubs': 'UBS'}\n",
    "#     company_ticker = company_tickers[company]\n",
    "#     dataset = dataset[dataset['ticker'] == company_ticker]\n",
    "#     company_users = users[company]\n",
    "#     return dataset[dataset['user'].isin(company_users)].groupby('user').last()\n",
    "\n",
    "# regress_data = {company_name: condition(company_name, company_data) for company_name, company_data in data.items()}\n",
    "\n",
    "# lehman = regress_data['leh']\n",
    "# lehman['is_lehman'] = 1\n",
    "\n",
    "# non_lehman = pd.concat([regress_data['db'], regress_data['gs'], regress_data['ms'], regress_data['ubs']])\n",
    "# non_lehman['is_lehman'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([lehman, non_lehman])\n",
    "\n",
    "index = all_data[all_data.birth.isin(['None', '2000'])].index\n",
    "all_data.loc[index, ['birth']] = '1976'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "informative_skills = ['Operations Management', 'Insurance', 'Business Development', 'Product Management',  '-1']\n",
    "\n",
    "not_informative = ~all_data.primary.isin(informative_skills)\n",
    "\n",
    "all_data.loc[not_informative, 'primary'] = 0\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X = all_data[['birth', 'gender', 'primary', 'education', 'elite']].copy()\n",
    "X['education'] = X['education'].apply(str)\n",
    "X['gender'] = X['gender'].apply(str)\n",
    "X['birth'] = X['birth'].astype(int)\n",
    "X['elite'] = X['elite'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y = all_data['is_lehman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.discrete.discrete_model as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(data=X, drop_first=True)\n",
    "X = sm.tools.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "logit = sm.Logit(y, X)\n",
    "\n",
    "results = logit.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "all_data['propensity'] = results.predict(X)\n",
    "all_data['job_category'] = all_data.user.apply(lambda x: full_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "user_to_propensity = dict(zip(all_data.user, all_data.propensity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lehman = all_data[all_data['is_lehman'] == 1]\n",
    "non_lehman = all_data[all_data['is_lehman'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_closest(row):\n",
    "    role = row.job_category\n",
    "    score = row.propensity\n",
    "    others_by_role = non_lehman[non_lehman.job_category == role].set_index('user')\n",
    "    return np.absolute(others_by_role['propensity'] - score).idxmin() \n",
    "\n",
    "\n",
    "lehman['match'] = lehman.apply(get_closest, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lehman['match_propensity'] = lehman.match.apply(lambda x : user_to_propensity[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#lehman.to_csv('./Deliverables/lehman_matches_job_titles_skills.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching on skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "drop = ['length','name','industry',\n",
    "            'primary_weight', 'secondary', 'secondary_weight', 'elite_education',\n",
    "            'city', 'country', '.', '??',\n",
    "            '/', 'department', 'exchange',\n",
    "            'public', 'location_company',\n",
    "            'major', 'department', 'FIGI', 'last_update','company','normalized_company','educational','degree']\n",
    "    \n",
    "skills_data = {company_name : company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}\n",
    "skills_data = {company_name: job_2008(company_name, company_data) for company_name, company_data in skills_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "non_lehman =  pd.concat([skills_data['db'], skills_data['gs'], skills_data['ms'], skills_data['ubs']])\n",
    "non_lehman['is_lehman'] = 0\n",
    "\n",
    "lehman = skills_data['leh'].copy()\n",
    "lehman['is_lehman'] = 1\n",
    "all_data = pd.concat([non_lehman, lehman])\n",
    "all_data['primary'] = all_data['primary'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "skills = list(all_data.primary.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# p_lehman = len(lehman) / len(all_data)\n",
    "# p_other = 1 - p_lehman\n",
    "\n",
    "# entropy_parent = - (p_lehman * np.log2(p_lehman) + p_other * np.log2(p_other))\n",
    "\n",
    "# split_fin = all_data[all_data.primary == 'Banking and Finance']\n",
    "# split_non_fin = all_data[~(all_data.primary == 'Banking and Finance')]\n",
    "\n",
    "# #finance skill split\n",
    "# p_fin_lehman = sum(split_fin.is_lehman)/len(split_fin)\n",
    "# p_fin_other = 1 - p_fin_lehman\n",
    "\n",
    "# entropy_fin = - (p_fin_lehman * np.log2(p_fin_lehman) + p_fin_other * np.log2(p_fin_other))\n",
    "\n",
    "# #non-finance skill split\n",
    "# p_non_fin_lehman = sum(split_non_fin.is_lehman)/len(split_non_fin)\n",
    "# p_non_fin_other = 1 - p_non_fin_lehman\n",
    "\n",
    "# entropy_non_fin = - (p_non_fin_lehman * np.log2(p_non_fin_lehman) + p_non_fin_other * np.log2(p_non_fin_other))\n",
    "\n",
    "# # [Weighted avg]Entropy(children) = \n",
    "# # (no. of examples in left child node) / (total no. of examples in parent node) * (entropy of left node) \n",
    "# # + \n",
    "# # (no. of examples in right child node)/ (total no. of examples in parent node) * (entropy of right node)\n",
    "\n",
    "# n = len(all_data)\n",
    "# left = len(split_fin)\n",
    "# right = len(split_non_fin)\n",
    "# entropy_split = left/n * entropy_fin + right/n * entropy_non_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# p_lehman = len(lehman) / len(all_data)\n",
    "# p_other = 1 - p_lehman\n",
    "# entropy_parent = - (p_lehman * np.log2(p_lehman) + p_other * np.log2(p_other))\n",
    "# n = len(all_data)\n",
    "\n",
    "# IG = []\n",
    "# for skill in skills:\n",
    "#     split = all_data[all_data.primary == skill]\n",
    "#     split_no = all_data[~(all_data.primary == skill)]\n",
    "    \n",
    "#     #look at people with the skill first\n",
    "#     p_split_lehman = sum(split.is_lehman)/len(split)\n",
    "#     p_split_other = 1 - p_split_lehman\n",
    "\n",
    "#     entropy_split = - (p_split_lehman * np.log2(p_split_lehman) + p_split_other * np.log2(p_split_other))\n",
    "    \n",
    "#     #look at people without the skill next\n",
    "#     p_no_lehman = sum(split_no.is_lehman)/len(split_no)\n",
    "#     p_no_other = 1 - p_no_lehman\n",
    "\n",
    "#     entropy_no_split = - (p_no_lehman * np.log2(p_no_lehman) + p_no_other * np.log2(p_no_other))\n",
    "    \n",
    "#     #weight by number in each split\n",
    "#     left = len(split)\n",
    "#     right = len(split_no)\n",
    "#     entropy_children = left/n * entropy_split + right/n * entropy_no_split\n",
    "    \n",
    "#     IG.append(entropy_parent - entropy_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#all_data = all_data[~(all_data['primary'] == '-1')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# p_lehman = len(lehman) / len(all_data)\n",
    "# p_other = 1 - p_lehman\n",
    "entropy_parent = - (p_lehman * np.log2(p_lehman) + p_other * np.log2(p_other))\n",
    "n = len(all_data)\n",
    "\n",
    "\n",
    "IG = []\n",
    "for skill in skills:\n",
    "    split = all_data[all_data.primary == skill]\n",
    "    split_no = all_data[~(all_data.primary == skill)]\n",
    "    \n",
    "    #look at people with the skill first\n",
    "#     p_split_lehman = sum(split.is_lehman)/len(split)\n",
    "#     p_split_other = 1 - p_split_lehman\n",
    "\n",
    "    #entropy_split = - (p_split_lehman * np.log2(p_split_lehman) + p_split_other * np.log2(p_split_other))\n",
    "    split_lehman = sum(split.is_lehman)\n",
    "    split_other = len(split) - split_lehman\n",
    "    entropy_split = st.entropy([split_lehman, split_other],base=2)\n",
    "    \n",
    "    #look at people without the skill next\n",
    "#     p_no_lehman = sum(split_no.is_lehman)/len(split_no)\n",
    "#     p_no_other = 1 - p_no_lehman\n",
    "\n",
    "    #entropy_no_split = - (p_no_lehman * np.log2(p_no_lehman) + p_no_other * np.log2(p_no_other))\n",
    "    no_lehman = sum(split_no.is_lehman)\n",
    "    no_other = len(split_no) - no_lehman\n",
    "    entropy_no_split = st.entropy([no_lehman, no_other],base=2)\n",
    "    \n",
    "    #weight by number in each split\n",
    "    left = len(split)\n",
    "    right = len(split_no)\n",
    "    entropy_children = left/n * entropy_split + right/n * entropy_no_split\n",
    "    \n",
    "    IG.append(entropy_parent - entropy_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "best_with_missing = np.flip(np.argsort(IG))\n",
    "\n",
    "best_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# best_with_out_missing = np.flip(np.argsort(IG))\n",
    "\n",
    "# best_with_out_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion breaks with all matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "drop = ['length', 'gender', 'primary',\n",
    "        'primary_weight', 'secondary', 'secondary_weight',\n",
    "        'city', 'country', 'education', 'elite', '.', '??',\n",
    "        '/', 'department', 'exchange',\n",
    "        'public', 'location_company', 'educational', 'degree', 'elite_education',\n",
    "        'major', 'department', 'FIGI', 'last_update', 'name', 'birth','end', 'role',\n",
    "        'company','normalized_company','industry']\n",
    "\n",
    "breaks_data = {company_name : company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lehman_to_match = dict(zip(lehman.user, lehman.match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lehman = breaks_data['leh'].copy()\n",
    "\n",
    "lehman['match'] = lehman.user.apply(lambda x : lehman_to_match[x])\n",
    "\n",
    "prop_breaks('leh', lehman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "matches = list(lehman.match.unique())\n",
    "\n",
    "non_lehman =  pd.concat([breaks_data['db'], breaks_data['gs'], breaks_data['ms'], breaks_data['ubs']])\n",
    "\n",
    "non_lehman_matches = non_lehman[non_lehman.user.isin(matches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "date_2008 = pd.to_datetime('2008-01-01')\n",
    "# look only at data after 2008\n",
    "after_2008 = non_lehman_matches[non_lehman_matches['start'] > date_2008]\n",
    "# groupby user, aggregate by looking at the ticker and seeing if the person has had any time off\n",
    "y = after_2008.groupby('user').ticker.agg(lambda x: any((x == 'TIME_OFF') | (x == 'MISSING')))\n",
    "\n",
    "w = y.to_frame()\n",
    "\n",
    "r = w[w.ticker == True]\n",
    "\n",
    "r = list(r.index)\n",
    "non_lehman_matches.loc['took_break'] = False\n",
    "\n",
    "non_lehman_matches.loc[non_lehman_matches.user.isin(r), 'took_break'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "user_to_break = dict(zip(non_lehman_matches.user, non_lehman_matches.took_break))\n",
    "\n",
    "x = lehman.groupby('user').first()\n",
    "\n",
    "x['break'] = x.match.apply(lambda x: user_to_break[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sum(x['break'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#x.to_csv('./Deliverables/matched_breaks.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
