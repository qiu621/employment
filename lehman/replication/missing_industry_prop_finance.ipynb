{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def main(csv_file):\n",
    "    \"\"\"\n",
    "    Given an appropriate csv_file, output the relevant columns.\n",
    "\n",
    "    Returns df with columns [user, start, end, normalized_company, industry]\n",
    "    \"\"\"\n",
    "\n",
    "    employ_data = pd.read_csv(csv_file, sep=\"\\t\", header=None,\n",
    "                              names=[i for i in range(34)], low_memory=False)\n",
    "    # column info from taxonomy file\n",
    "    name = ['user', 'name', 'birth', 'gender', 'primary',\n",
    "            'primary_weight', 'secondary', 'secondary_weight',\n",
    "            'city', 'country', 'education', 'elite', 'start',\n",
    "            '.', 'end', '??', '/', 'length', 'role', 'department',\n",
    "            'company', 'normalized_company', 'ticker', 'exchange',\n",
    "            'public', 'location_company', 'industry', 'educational',\n",
    "            'degree', 'elite_education', 'major', 'department', 'FIGI',\n",
    "            'last_update']\n",
    "#     drop = ['length', 'gender', 'primary',\n",
    "#         'primary_weight', 'secondary', 'secondary_weight',\n",
    "#         'city', 'country', 'education', 'elite', '.', '??',\n",
    "#         '/', 'department', 'exchange',\n",
    "#         'public', 'location_company', 'educational', 'degree', 'elite_education',\n",
    "#         'major', 'department', 'FIGI', 'last_update']\n",
    "    employ_data.columns = name\n",
    "    return employ_data\n",
    "\n",
    "\n",
    "# data without datetime features, and none values for some dates\n",
    "raw_data = {'db': main('./Data/DB_profiles.csv'),\n",
    "            'gs': main('./Data/GS_profiles.csv'),\n",
    "            'leh': main('./Data/LEH_profiles.csv'),\n",
    "            'ms': main('./Data/MS_profiles.csv'),\n",
    "            'ubs': main('./Data/UBS_profiles.csv')\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def standardize_dates(company, missing_start = '1900-01-01', missing_end = '2018-01-01'):\n",
    "    \"\"\"\n",
    "    Converts start date and end date to datetime objects, and converts None values to the specified missing \n",
    "    dates.\n",
    "\n",
    "    Returns the modified dataframe\n",
    "    \"\"\"\n",
    "    company_data = raw_data[company].copy()\n",
    "    company_data['start'] = company_data['start'].str.replace('None', missing_start)\n",
    "    company_data['end'] = company_data['end'].str.replace('None', missing_end)\n",
    "    company_data['start'] = pd.to_datetime(company_data['start'])\n",
    "    company_data['end'] = pd.to_datetime(company_data['end'])\n",
    "    return company_data\n",
    "\n",
    "\n",
    "# set up dictionary to hold data for each company\n",
    "\n",
    "for company in raw_data.keys():\n",
    "    raw_data[company] = standardize_dates(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_users(company_name, company_data, worked_date = '2008-01-01', missing_start = '1900-01-01', missing_end = '2018-01-01'):\n",
    "    \"\"\"\"\n",
    "    Returns the users who worked at a given company on worked_date, that does not have both start and\n",
    "    end dates missing\n",
    "    \n",
    "    worked_date: string specifying the date on which to extract employees from. \n",
    "                 Must be coercible into a datetime object\n",
    "    missing_start: default value for missing start dates\n",
    "    missing_end: default value for missing end dates\n",
    "    \"\"\"\n",
    "    worked_date = pd.to_datetime(worked_date)\n",
    "    missing_start = pd.to_datetime(missing_start)\n",
    "    missing_end = pd.to_datetime(missing_end)\n",
    "    x = company_data\n",
    "    \n",
    "    company_tickers = {'db': 'DB', 'leh': 'LEH', 'gs': 'GS', 'ms': 'MS^E', 'ubs': 'UBS'}\n",
    "    # conditions: start and end not both missing, worked before/after 2008-01-01, ticker matches company\n",
    "    mask = ~((x['start'] == missing_start) & (x['end'] == missing_end)) & \\\n",
    "        (x['start'] < worked_date) & \\\n",
    "        (x['end'] > worked_date) & \\\n",
    "        (x['ticker'] == company_tickers[company_name])\n",
    "    return company_data[mask]['user'].unique()\n",
    "\n",
    "\n",
    "# gets the user_ids within each company that match the conditioning, before and and after\n",
    "#   2008-01-01, exclusive\n",
    "users = {company_name: get_users(company_name, company_data) for company_name, company_data in raw_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for company, company_data in raw_data.items():\n",
    "    company_users = users[company]\n",
    "    data[company] = company_data[company_data['user'].isin(company_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# read all the csv files\n",
    "profile = pd.read_csv('./Data/profile_industry_mappings.csv', header=None, names=[i for i in range(5)], dtype={4: str})\n",
    "profile.drop([0, 2], axis='columns', inplace=True)\n",
    "profile.rename(mapper={1: 'company', 3: 'norm', 4: \"ind\"}, axis='columns', inplace=True)\n",
    "\n",
    "mturk = pd.read_csv('./Data/industries_MTurkers_20170711.csv', header=None, encoding='latin-1')\n",
    "mturk.drop([1], axis='columns', inplace=True)\n",
    "mturk.rename(mapper={0: 'company', 2: \"ind\"}, axis='columns', inplace=True)\n",
    "\n",
    "finance = pd.read_csv('./Data/Finance.csv', dtype={'Industry': str})\n",
    "finance.drop([finance.columns[0], finance.columns[2], finance.columns[4]], axis='columns', inplace=True)\n",
    "finance.rename(mapper={'Normalized Company Name': 'norm', 'Industry': \"ind\"}, axis='columns', inplace=True)\n",
    "\n",
    "manual = pd.read_csv('./Data/manual_industry_mappings.csv', encoding='latin-1', header=None, dtype={2: str})\n",
    "manual.drop([1], axis='columns', inplace=True)\n",
    "manual.rename(mapper={0: 'norm', 2: \"ind\"}, axis='columns', inplace=True)\n",
    "\n",
    "industries_2019 = pd.read_csv('./Data/missing_industries_2019.csv', header=None, dtype={2: str})\n",
    "industries_2019 = industries_2019[~(industries_2019[1] == 1)].copy()\n",
    "\n",
    "industries_2019.drop([1], axis = 'columns', inplace = True)\n",
    "industries_2019.rename(mapper={0: 'company', 2: \"ind\"}, axis='columns', inplace=True)\n",
    "industries_2019 = industries_2019[~pd.isnull(industries_2019.ind)].copy()\n",
    "\n",
    "#mturk industry is given as \"ind_x\", profile industry is given as \"ind_y\"\n",
    "company_comb = pd.merge(mturk, profile, on='company', how='outer')\n",
    "#prioritize mturk data\n",
    "company_comb['combined'] = company_comb['ind_x'].combine_first(company_comb['ind_y'])\n",
    "\n",
    "#mturk industry is given as \"ind\", profile industry is given as \"combined\"\n",
    "company_comb = pd.merge(industries_2019, company_comb, on='company', how='outer')\n",
    "#prioritize manual entry data\n",
    "company_comb['combined'] = company_comb['ind'].combine_first(company_comb['combined'])\n",
    "\n",
    "#merge manual and finance files, prioritizing manual\n",
    "norm_comb = pd.merge(manual, finance, on = 'norm', how = 'outer')\n",
    "norm_comb['combined'] = norm_comb['ind_x'].combine_first(norm_comb['ind_y'])\n",
    "#merge manual/finance and profile[norm], prioritizing manual/finance\n",
    "norm_comb = pd.merge(norm_comb, profile, on = 'norm', how = 'outer')\n",
    "norm_comb['combined'] = norm_comb['combined'].combine_first(norm_comb['ind'])\n",
    "\n",
    "# convert the columns of the aggredated dataframe into a dictionary where the key is the company name\n",
    "# and the value is the industry code\n",
    "norm_mapping = dict(zip(norm_comb.norm, norm_comb.combined))\n",
    "company_mapping = dict(zip(company_comb.company, company_comb.combined))\n",
    "# set the default value if the company is not found to NaN\n",
    "norm_mapping = defaultdict(lambda: np.NaN, norm_mapping)\n",
    "company_mapping = defaultdict(lambda: np.NaN, company_mapping)\n",
    "\n",
    "def filter_manual(company_data):\n",
    "    \"\"\"\n",
    "    Adds industry labels to entries that don't have one, based on the manual industry data\n",
    "    \"\"\"\n",
    "    company_data = company_data.copy()\n",
    "    # convert to lowercase for more accurate matching\n",
    "    company_data['normalized_company_lower'] = company_data['normalized_company'].str.lower()\n",
    "    company_data['company_lower'] = company_data['company'].str.lower()\n",
    "    # apply norm_mapping and company_mapping to upper and lower case versions\n",
    "    company_data['company_mapped'] = company_data['company'].apply(lambda y: company_mapping[y])\n",
    "    company_data['normalized_company_mapped'] = company_data['normalized_company'].apply(lambda y: norm_mapping[y])\n",
    "    company_data['company_lower_mapped'] = company_data['normalized_company_lower'].apply(lambda y: norm_mapping[y])\n",
    "    company_data['normalized_company_lower_mapped'] = company_data['normalized_company_lower'].apply(lambda y: norm_mapping[y])\n",
    "    # combines all mappings. Prioritize Existing industry code > MTurk/profle(company) > \n",
    "    # manual/finance/profile(normalized_company) > manual/finance/profile(normalized_company_lower) \n",
    "    company_data['industry_two'] = company_data['industry'].combine_first(company_data['company_mapped'])\n",
    "    company_data['industry_three'] = company_data['industry_two'].combine_first(company_data['normalized_company_mapped'])\n",
    "    company_data['industry_four'] = company_data['industry_three'].combine_first(company_data['company_lower_mapped'])\n",
    "    company_data['industry_five'] = company_data['industry_four'].combine_first(company_data['normalized_company_lower_mapped'])\n",
    "    company_data['industry'] = company_data['industry_five']\n",
    "    # drop the temporary columns\n",
    "    company_data.drop(['normalized_company_lower', 'company_lower', 'company_mapped', 'normalized_company_mapped', 'company_lower_mapped','normalized_company_lower_mapped', 'industry_two', 'industry_three', 'industry_four','industry_five'], axis=1, inplace=True)\n",
    "    return company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def mask(company_data):\n",
    "    \"\"\"\n",
    "    Return values in the time range with start before '2016-1-1' and end after '2016-1-1'.\n",
    "\n",
    "    Excludes values that don't have a start or end time, or are educational.\n",
    "    \"\"\"\n",
    "    mask = (company_data['start'] <= pd.to_datetime('2016-1-1')) & (company_data['end'] >= pd.to_datetime('2016-1-1')) & ~((company_data['start'] == pd.to_datetime('1900-01-01')) & (company_data['end'] == pd.to_datetime('2018-01-01'))) & (~company_data['ticker'].isin(['UNIVERSITY', 'SCHOOL']) & ~(company_data.educational))\n",
    "    #& ~(pd.isnull(company_data.industry))\n",
    "    return company_data[mask]\n",
    "\n",
    "\n",
    "def filter_and_mask(company_data):\n",
    "    # combines filter and mask\n",
    "    filtered = filter_manual(company_data)\n",
    "    return mask(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# def mask_missing(company_data):\n",
    "#     \"\"\"\n",
    "#     Return values in the time range with start before '2016-1-1' and end after '2016-1-1'.\n",
    "\n",
    "#     Excludes values that don't have a start or end time.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     mask = (company_data['start'] < pd.to_datetime('2016-1-1')) & (company_data['end'] > pd.to_datetime('2016-1-1')) & (company_data['start'] != pd.to_datetime('1900-01-01')) & (~company_data['ticker'].isin(['UNIVERSITY', 'TIME_OFF', 'SCHOOL', 'MISSING', 'FREELANCE', 'UNEMPLOYED', 'RETIRED']))\n",
    "#     return company_data[mask]\n",
    "\n",
    "# all_data =  pd.concat(raw_data.values())\n",
    "\n",
    "# company_data = mask_missing(all_data).copy()\n",
    "\n",
    "# company_data['normalized_company_lower'] = company_data['normalized_company'].str.lower()\n",
    "# company_data['company_lower'] = company_data['company'].str.lower()\n",
    "# # apply norm_mapping and company_mapping to upper and lower case versions\n",
    "# company_data['company_mapped'] = company_data['company'].apply(lambda y: company_mapping[y])\n",
    "# company_data['normalized_company_mapped'] = company_data['normalized_company'].apply(lambda y: norm_mapping[y])\n",
    "# company_data['company_lower_mapped'] = company_data['normalized_company_lower'].apply(lambda y: norm_mapping[y])\n",
    "# company_data['normalized_company_lower_mapped'] = company_data['normalized_company_lower'].apply(lambda y: norm_mapping[y])\n",
    "# # combines all mappings. Prioritize Existing industry code > MTurk/profle(company) > \n",
    "# # manual/finance/profile(normalized_company) > manual/finance/profile(normalized_company_lower) \n",
    "# company_data['industry_two'] = company_data['industry'].combine_first(company_data['company_mapped'])\n",
    "# company_data['industry_three'] = company_data['industry_two'].combine_first(company_data['normalized_company_mapped'])\n",
    "# company_data['industry_four'] = company_data['industry_three'].combine_first(company_data['company_lower_mapped'])\n",
    "# company_data['industry_five'] = company_data['industry_four'].combine_first(company_data['normalized_company_lower_mapped'])\n",
    "# company_data['industry'] = company_data['industry_five']\n",
    "# # drop the temporary columns\n",
    "# company_data.drop(['company_mapped', 'normalized_company_mapped', 'company_lower_mapped','normalized_company_lower_mapped', 'industry_two', 'industry_three', 'industry_four','industry_five'], axis=1, inplace=True)\n",
    "\n",
    "# x = company_data.groupby('user').last()\n",
    "# most_recent_missing = x[pd.isnull(x['industry'])]\n",
    "\n",
    "#most_recent_missing['company'].value_counts().to_csv('./Deliverables/missing_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching with missing job entries as of 2016-1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# begin matching on job titles, prepare data by dropping irrelevant names\n",
    "drop = ['length', 'gender', 'primary',\n",
    "        'primary_weight', 'secondary', 'secondary_weight',\n",
    "        'city', 'country', 'education', 'elite', '.', '??',\n",
    "        '/', 'department', 'exchange',\n",
    "        'public', 'location_company', 'educational', 'degree', 'elite_education',\n",
    "        'major', 'department', 'FIGI', 'last_update', 'industry', 'birth', 'company']\n",
    "matching_data = {company_name: company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}\n",
    "\n",
    "\n",
    "def job_2008(company_name, company_data):\n",
    "    \"\"\"\"\n",
    "    Return each user's job at the given company as of 2008-01-01\n",
    "    \"\"\"\n",
    "    date_2008 = pd.to_datetime('2008-01-01')\n",
    "    missing_start = pd.to_datetime('1900-01-01')\n",
    "    missing_end = pd.to_datetime('2018-01-01')\n",
    "\n",
    "    company_tickers = {'db': 'DB', 'leh': 'LEH', 'gs': 'GS', 'ms': 'MS^E', 'ubs': 'UBS'}\n",
    "\n",
    "    x = company_data\n",
    "    mask = ~((x['start'] == missing_start) & (x['end'] == missing_end)) & \\\n",
    "           (x['start'] < date_2008) & \\\n",
    "           (x['end'] > date_2008) & \\\n",
    "           (x['ticker'] == company_tickers[company_name])\n",
    "    return company_data[mask]\n",
    "\n",
    "\n",
    "job_as_of_2008 = {company_name: job_2008(company_name, company_data) for company_name, company_data in\n",
    "                  matching_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat(job_as_of_2008.values())\n",
    "# only person missing a role in the entire data set\n",
    "all_data = all_data.drop(11512)\n",
    "\n",
    "# begin extracting job titles\n",
    "directors = set(all_data[(all_data.role.str.contains(r'director|MD,md', case=False))\n",
    "                         | (all_data.role.str.match(r'ed|md', case=False))].user)\n",
    "all_roles = directors.copy()\n",
    "\n",
    "analysts = set(all_data[all_data.role.str.contains('analyst|Anaylst', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(analysts)\n",
    "\n",
    "vps = set(all_data[all_data.role.str.contains('president|vp', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(vps)\n",
    "\n",
    "assocs = set(all_data[all_data.role.str.contains('associate', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(assocs)\n",
    "\n",
    "accountants = set(\n",
    "    all_data[all_data.role.str.contains('accountant|account executive|accounting', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(accountants)\n",
    "\n",
    "consultants = set(all_data[all_data.role.str.contains('consultant', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(consultants)\n",
    "\n",
    "missing = set(all_data[all_data.role.str.match(r'-|\\?|\\.', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(missing)\n",
    "\n",
    "developers = set(\n",
    "    all_data[all_data.role.str.contains(r'developer|engineer|system administrator', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(developers)\n",
    "\n",
    "interns = set(all_data[all_data.role.str.contains('intern|trainee|apprentice', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(interns)\n",
    "\n",
    "specialists = set(\n",
    "    all_data[all_data.role.str.contains('specialist|administrator|research|expert', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(specialists)\n",
    "\n",
    "sales = set(all_data[all_data.role.str.contains('sales', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(sales)\n",
    "\n",
    "traders = set(all_data[all_data.role.str.contains(r'trader|trading|Portfolio Management', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(traders)\n",
    "\n",
    "bankers = set(all_data[all_data.role.str.contains(r'banking|banker|finance', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(bankers)\n",
    "\n",
    "controllers = set(all_data[all_data.role.str.contains('controller', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(controllers)\n",
    "\n",
    "partners = set(all_data[all_data.role.str.contains('partner', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(partners)\n",
    "\n",
    "counsels = set(all_data[all_data.role.str.contains('counsel', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(counsels)\n",
    "\n",
    "recruiters = set(all_data[all_data.role.str.contains('recruiter|human resources', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(recruiters)\n",
    "\n",
    "advisors = set(all_data[all_data.role.str.contains('advisor|adviseur', case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(advisors)\n",
    "\n",
    "assistants = set(\n",
    "    all_data[all_data.role.str.contains('assistant|support|services|receptionist', case=False)].user).difference(\n",
    "    all_roles)\n",
    "all_roles = all_roles.union(assistants)\n",
    "\n",
    "managers = set(all_data[all_data.role.str.contains(\n",
    "    r'manager|supervisor|team lead|head|lead|coordinator|representative|process executive',\n",
    "    case=False)].user).difference(all_roles)\n",
    "all_roles = all_roles.union(managers)\n",
    "\n",
    "others = set(all_data.user).difference(all_roles)\n",
    "\n",
    "# zip all sets and all job title names\n",
    "all_sets = [directors, analysts, vps, assocs, advisors, assistants, consultants, managers, missing, developers, interns,\n",
    "            specialists, sales, traders, bankers, controllers, partners, counsels, recruiters, accountants, others]\n",
    "job_titles = ['director', 'analyst', 'vp', 'assoc', 'advisor', 'assistant', 'consultant', 'manager', 'missing',\n",
    "              'developer', 'intern', 'specialist', 'sale', 'trader', 'banker', 'controller', 'partner', 'counsel',\n",
    "              'recruiter', 'accountant', 'other']\n",
    "\n",
    "zipped = list(zip(all_sets, job_titles))\n",
    "\n",
    "\n",
    "def to_dict(dictionary, users, job_title):\n",
    "    \"\"\"Map users to job_title in the given dictionary\"\"\"\n",
    "    for user in users:\n",
    "        dictionary.update({user: job_title})\n",
    "\n",
    "\n",
    "full_mapping = {}\n",
    "[to_dict(full_mapping, x, y) for x, y in zipped]\n",
    "full_mapping.update({'c0a3eb6a-59db-3a30-8a39-99a7cc8b9ce1': 'specialist'})\n",
    "full_mapping.update({'5f425323-1cdf-3e81-a08e-35b483c42da9': 'missing'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.discrete.discrete_model as sm\n",
    "# prepare data for regression by dropping irrelevant names\n",
    "drop = ['length', 'name',\n",
    "        'primary_weight', 'secondary', 'secondary_weight', 'elite_education',\n",
    "        'city', 'country', '.', '??',\n",
    "        '/', 'department', 'exchange',\n",
    "        'public', 'location_company',\n",
    "        'major', 'department', 'FIGI', 'last_update', 'degree']\n",
    "\n",
    "regression_data = {company_name: company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#additional step of filtering out those who don't have a job entry on 2016-1-1\n",
    "all_data_2016 = pd.concat(regression_data.values())\n",
    "employed_2016 = filter_and_mask(all_data_2016)\n",
    "employ_2016_users = list(employed_2016.user.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "regression_data = {company_name: job_2008(company_name, company_data) for company_name, company_data in\n",
    "                   regression_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "non_lehman = pd.concat([regression_data['db'], regression_data['gs'], regression_data['ms'], regression_data['ubs']])\n",
    "non_lehman['is_lehman'] = 0\n",
    "\n",
    "lehman = regression_data['leh'].copy()\n",
    "lehman['is_lehman'] = 1\n",
    "\n",
    "all_data = pd.concat([lehman, non_lehman])\n",
    "\n",
    "all_data = all_data[all_data.user.isin(employ_2016_users)]\n",
    "\n",
    "# fill in missing births to the median date, 1976\n",
    "index = all_data[all_data.birth.isin(['None', '2000'])].index\n",
    "all_data.loc[index, ['birth']] = '1976'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "all_data = all_data.groupby('user').first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## import pickle\n",
    "# with open('./Data/employed_2016.csv', 'wb') as handle:\n",
    "#     pickle.dump(employ_2016_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informative skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "skills = list(all_data.primary.value_counts().index)\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "#all_data = all_data[~(all_data['primary'] == '-1')].copy()\n",
    "p_lehman = len(all_data[all_data.is_lehman == 1]) / len(all_data)\n",
    "p_other = 1 - p_lehman\n",
    "entropy_parent = - (p_lehman * np.log2(p_lehman) + p_other * np.log2(p_other))\n",
    "n = len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "IG = []\n",
    "for skill in skills:\n",
    "    split = all_data[all_data.primary == skill]\n",
    "    split_no = all_data[~(all_data.primary == skill)]\n",
    "    \n",
    "    #look at people with the skill first\n",
    "#     p_split_lehman = sum(split.is_lehman)/len(split)\n",
    "#     p_split_other = 1 - p_split_lehman\n",
    "\n",
    "    #entropy_split = - (p_split_lehman * np.log2(p_split_lehman) + p_split_other * np.log2(p_split_other))\n",
    "    split_lehman = sum(split.is_lehman)\n",
    "    split_other = len(split) - split_lehman\n",
    "    entropy_split = st.entropy([split_lehman, split_other],base=2)\n",
    "    \n",
    "    #look at people without the skill next\n",
    "#     p_no_lehman = sum(split_no.is_lehman)/len(split_no)\n",
    "#     p_no_other = 1 - p_no_lehman\n",
    "\n",
    "    #entropy_no_split = - (p_no_lehman * np.log2(p_no_lehman) + p_no_other * np.log2(p_no_other))\n",
    "    no_lehman = sum(split_no.is_lehman)\n",
    "    no_other = len(split_no) - no_lehman\n",
    "    entropy_no_split = st.entropy([no_lehman, no_other],base=2)\n",
    "    \n",
    "    #weight by number in each split\n",
    "    left = len(split)\n",
    "    right = len(split_no)\n",
    "    entropy_children = left/n * entropy_split + right/n * entropy_no_split\n",
    "    \n",
    "    IG.append(entropy_parent - entropy_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sorted_ig_indices = np.flip(np.argsort(IG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "skills_by_ig = [skills[index] for index in sorted_ig_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Operations Management',\n",
       " 'Insurance',\n",
       " 'Business Development',\n",
       " 'Product Management',\n",
       " '-1',\n",
       " 'Human Resources (Senior)',\n",
       " 'Real Estate',\n",
       " 'Mobile Telecommunications',\n",
       " 'Software Engineering',\n",
       " 'Technical Product Management',\n",
       " 'Non-Profit and Community',\n",
       " 'Banking and Finance',\n",
       " 'Administration',\n",
       " 'Data Analysis',\n",
       " 'Industrial Management',\n",
       " 'Musical Production',\n",
       " 'Web Development',\n",
       " 'Accounting and Auditing',\n",
       " 'Construction Management',\n",
       " 'Electrical Engineering',\n",
       " 'Social Media and Communications',\n",
       " 'Sales Management',\n",
       " 'Military',\n",
       " 'Video and Film Production',\n",
       " 'Sales',\n",
       " 'Logistics',\n",
       " 'Middle Management',\n",
       " 'Web Design',\n",
       " 'Manufacturing and Process Management',\n",
       " 'IT Management and Support',\n",
       " 'Graphic Design',\n",
       " 'Digital Marketing',\n",
       " 'CRM and Sales Management',\n",
       " 'Energy, Oil, and Gas',\n",
       " 'Retail and Fashion',\n",
       " 'Human Resources (Junior)',\n",
       " 'Education',\n",
       " 'Public Policy',\n",
       " 'Personal Coaching',\n",
       " 'Hospitality',\n",
       " 'Healthcare',\n",
       " 'Legal',\n",
       " 'Visual Design',\n",
       " 'Pharmaceutical',\n",
       " 'Recruiting']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_by_ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366821\n",
      "         Iterations 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# data deemed informative by information gain. Missing is coded as '-1'\n",
    "informative_skills = skills_by_ig[0:5]\n",
    "#convert uninformative skills to '0'\n",
    "not_informative = ~all_data.primary.isin(informative_skills)\n",
    "all_data.loc[not_informative, 'primary'] = 0\n",
    "all_data['job_category'] = all_data.user.apply(lambda x: full_mapping[x])\n",
    "\n",
    "# make sure typing is consistent for each category\n",
    "X = all_data[['birth', 'gender', 'primary', 'education', 'elite']].copy()\n",
    "X['education'] = X['education'].apply(str)\n",
    "X['gender'] = X['gender'].apply(str)\n",
    "X['birth'] = X['birth'].astype(int)\n",
    "X['elite'] = X['elite'].astype(int)\n",
    "X = pd.get_dummies(data=X, drop_first=True)\n",
    "X = sm.tools.add_constant(X)\n",
    "\n",
    "y = all_data['is_lehman']\n",
    "\n",
    "# regress y on X\n",
    "logit = sm.Logit(y, X)\n",
    "results = logit.fit(maxiter = 100)\n",
    "\n",
    "# get propensities\n",
    "all_data['propensity'] = results.predict(X)\n",
    "\n",
    "# Begin matching process. Map each user to its propensity\n",
    "user_to_propensity = dict(zip(all_data.user, all_data.propensity))\n",
    "\n",
    "# get lehman and non-lehman guys\n",
    "lehman = all_data[all_data['is_lehman'] == 1]\n",
    "non_lehman = all_data[all_data['is_lehman'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>is_lehman</td>    <th>  No. Observations:  </th>   <td> 78944</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td> 78927</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    16</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 20 Jan 2020</td> <th>  Pseudo R-squ.:     </th>   <td>0.01096</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>09:16:00</td>     <th>  Log-Likelihood:    </th>  <td> -28958.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -29279.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.929e-126</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                   <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                         <td>  -28.9814</td> <td>    3.147</td> <td>   -9.211</td> <td> 0.000</td> <td>  -35.148</td> <td>  -22.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>birth</th>                         <td>    0.0137</td> <td>    0.002</td> <td>    8.636</td> <td> 0.000</td> <td>    0.011</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>elite</th>                         <td>    0.2183</td> <td>    0.032</td> <td>    6.799</td> <td> 0.000</td> <td>    0.155</td> <td>    0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender_1</th>                      <td>   -0.2492</td> <td>    0.032</td> <td>   -7.734</td> <td> 0.000</td> <td>   -0.312</td> <td>   -0.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender_2</th>                      <td>   -0.1641</td> <td>    0.027</td> <td>   -6.092</td> <td> 0.000</td> <td>   -0.217</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>primary_-1</th>                    <td>   -0.0074</td> <td>    0.023</td> <td>   -0.315</td> <td> 0.753</td> <td>   -0.053</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>primary_Business Development</th>  <td>   -0.6814</td> <td>    0.139</td> <td>   -4.916</td> <td> 0.000</td> <td>   -0.953</td> <td>   -0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>primary_Insurance</th>             <td>   -1.8187</td> <td>    0.233</td> <td>   -7.808</td> <td> 0.000</td> <td>   -2.275</td> <td>   -1.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>primary_Operations Management</th> <td>   -1.0350</td> <td>    0.101</td> <td>  -10.244</td> <td> 0.000</td> <td>   -1.233</td> <td>   -0.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>primary_Product Management</th>    <td>   -2.0300</td> <td>    0.583</td> <td>   -3.483</td> <td> 0.000</td> <td>   -3.172</td> <td>   -0.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_1</th>                   <td>   -0.3046</td> <td>    0.115</td> <td>   -2.651</td> <td> 0.008</td> <td>   -0.530</td> <td>   -0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_2</th>                   <td>   -0.5507</td> <td>    0.197</td> <td>   -2.796</td> <td> 0.005</td> <td>   -0.937</td> <td>   -0.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_3</th>                   <td>    0.1103</td> <td>    0.164</td> <td>    0.671</td> <td> 0.502</td> <td>   -0.212</td> <td>    0.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_4</th>                   <td>    0.0533</td> <td>    0.027</td> <td>    1.958</td> <td> 0.050</td> <td>-6.06e-05</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_5</th>                   <td>   -0.0961</td> <td>    0.039</td> <td>   -2.451</td> <td> 0.014</td> <td>   -0.173</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_6</th>                   <td>    0.2129</td> <td>    0.039</td> <td>    5.515</td> <td> 0.000</td> <td>    0.137</td> <td>    0.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_7</th>                   <td>    0.1906</td> <td>    0.063</td> <td>    3.027</td> <td> 0.002</td> <td>    0.067</td> <td>    0.314</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              is_lehman   No. Observations:                78944\n",
       "Model:                          Logit   Df Residuals:                    78927\n",
       "Method:                           MLE   Df Model:                           16\n",
       "Date:                Mon, 20 Jan 2020   Pseudo R-squ.:                 0.01096\n",
       "Time:                        09:16:00   Log-Likelihood:                -28958.\n",
       "converged:                       True   LL-Null:                       -29279.\n",
       "Covariance Type:            nonrobust   LLR p-value:                2.929e-126\n",
       "=================================================================================================\n",
       "                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------\n",
       "const                           -28.9814      3.147     -9.211      0.000     -35.148     -22.814\n",
       "birth                             0.0137      0.002      8.636      0.000       0.011       0.017\n",
       "elite                             0.2183      0.032      6.799      0.000       0.155       0.281\n",
       "gender_1                         -0.2492      0.032     -7.734      0.000      -0.312      -0.186\n",
       "gender_2                         -0.1641      0.027     -6.092      0.000      -0.217      -0.111\n",
       "primary_-1                       -0.0074      0.023     -0.315      0.753      -0.053       0.039\n",
       "primary_Business Development     -0.6814      0.139     -4.916      0.000      -0.953      -0.410\n",
       "primary_Insurance                -1.8187      0.233     -7.808      0.000      -2.275      -1.362\n",
       "primary_Operations Management    -1.0350      0.101    -10.244      0.000      -1.233      -0.837\n",
       "primary_Product Management       -2.0300      0.583     -3.483      0.000      -3.172      -0.888\n",
       "education_1                      -0.3046      0.115     -2.651      0.008      -0.530      -0.079\n",
       "education_2                      -0.5507      0.197     -2.796      0.005      -0.937      -0.165\n",
       "education_3                       0.1103      0.164      0.671      0.502      -0.212       0.432\n",
       "education_4                       0.0533      0.027      1.958      0.050   -6.06e-05       0.107\n",
       "education_5                      -0.0961      0.039     -2.451      0.014      -0.173      -0.019\n",
       "education_6                       0.2129      0.039      5.515      0.000       0.137       0.289\n",
       "education_7                       0.1906      0.063      3.027      0.002       0.067       0.314\n",
       "=================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1974.6611015403323"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X.birth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "def get_closest(row):\n",
    "    # return user ID of closest match with the same job title\n",
    "    role = row.job_category\n",
    "    score = row.propensity\n",
    "    others_by_role = non_lehman[non_lehman.job_category == role].set_index('user')\n",
    "    return np.absolute(others_by_role['propensity'] - score).idxmin()\n",
    "\n",
    "\n",
    "# get closest match for each lehman guy\n",
    "lehman['match'] = lehman.apply(get_closest, axis=1)\n",
    "lehman['match_propensity'] = lehman.match.apply(lambda x: user_to_propensity[x])\n",
    "\n",
    "lehman_matches = lehman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>birth</th>\n",
       "      <th>gender</th>\n",
       "      <th>primary</th>\n",
       "      <th>education</th>\n",
       "      <th>elite</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>normalized_company</th>\n",
       "      <th>ticker</th>\n",
       "      <th>industry</th>\n",
       "      <th>educational</th>\n",
       "      <th>is_lehman</th>\n",
       "      <th>job_category</th>\n",
       "      <th>propensity</th>\n",
       "      <th>match</th>\n",
       "      <th>match_propensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>73495</td>\n",
       "      <td>ee4460d1-2bfd-3869-be44-353e1d8edf70</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2007-12-01</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>Assistant Vice President,vice president,assistant</td>\n",
       "      <td>Lehman Brothers / Barclays Capital</td>\n",
       "      <td>Lehman Brothers</td>\n",
       "      <td>LEH</td>\n",
       "      <td>522110</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>vp</td>\n",
       "      <td>0.114646</td>\n",
       "      <td>06f62a48-4126-326a-8f4c-8fc668c2fb80</td>\n",
       "      <td>0.114646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user birth  gender primary  education  \\\n",
       "73495  ee4460d1-2bfd-3869-be44-353e1d8edf70  1976       1      -1          4   \n",
       "\n",
       "       elite      start        end  \\\n",
       "73495  False 2007-12-01 2009-01-01   \n",
       "\n",
       "                                                    role  \\\n",
       "73495  Assistant Vice President,vice president,assistant   \n",
       "\n",
       "                                  company normalized_company ticker industry  \\\n",
       "73495  Lehman Brothers / Barclays Capital    Lehman Brothers    LEH   522110   \n",
       "\n",
       "       educational  is_lehman job_category  propensity  \\\n",
       "73495        False          1           vp    0.114646   \n",
       "\n",
       "                                      match  match_propensity  \n",
       "73495  06f62a48-4126-326a-8f4c-8fc668c2fb80          0.114646  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lehman[lehman.user == 'ee4460d1-2bfd-3869-be44-353e1d8edf70']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'('ee4460d1-2bfd-3869-be44-353e1d8edf70', slice(None, None, None))' is an invalid key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-a70d7de13911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlehman\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ee4460d1-2bfd-3869-be44-353e1d8edf70'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 )\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '('ee4460d1-2bfd-3869-be44-353e1d8edf70', slice(None, None, None))' is an invalid key"
     ]
    }
   ],
   "source": [
    "lehman['ee4460d1-2bfd-3869-be44-353e1d8edf70', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive proportion finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "drop = ['length', 'gender', 'primary',\n",
    "        'primary_weight', 'secondary', 'secondary_weight',\n",
    "        'city', 'country', 'education', 'elite', '.', '??',\n",
    "        '/', 'department', 'exchange',\n",
    "        'public', 'location_company', 'degree', 'elite_education',\n",
    "        'major', 'department', 'FIGI', 'last_update']\n",
    "\n",
    "finance_data = {company_name : company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}\n",
    "\n",
    "def prop_finance(company_data):\n",
    "    # exclude values with tickers in the categories\n",
    "    copy = company_data.copy()\n",
    "    users = company_data.groupby('user')\n",
    "    total_users = len(users)\n",
    "    # get each person's most recent job\n",
    "    recent_jobs = users.first()\n",
    "    # sum the people who stayed in finance industries\n",
    "    stayed_finance = sum(recent_jobs['industry'].str.startswith('52', na=False))\n",
    "    return stayed_finance, total_users, stayed_finance/total_users\n",
    "\n",
    "# Apply mappings for missing industries with the dictionary, then mask to look only at job entries that start before\n",
    "# '2016-1-1' and end after '2016-1-1'.\n",
    "filtered_data = {company_name: filter_and_mask(company_data) for company_name, company_data in finance_data.items()}\n",
    "\n",
    "# calculate the proportion that stayed in finance as of 2016-01-01\n",
    "prop_stayed_finance = {company_name: prop_finance(company_data) for company_name, company_data in filtered_data.items()}\n",
    "prop_stayed_finance\n",
    "\n",
    "# # same as before, but don't filter the data with the dictionary\n",
    "# filtered_data = {company_name: mask(company_data) for company_name, company_data in finance_data.items()}\n",
    "# prop_unfiltered = {company_name: prop_finance(company_data) for company_name, company_data in filtered_data.items()}\n",
    "# prop_unfiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matched proportion stayed in finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lehman_to_match = dict(zip(lehman_matches.user, lehman_matches.match))\n",
    "\n",
    "lehman = filtered_data['leh'].copy()\n",
    "lehman['match'] = lehman.user.apply(lambda x : lehman_to_match[x])\n",
    "\n",
    "matched_users = list(lehman.match.unique())\n",
    "\n",
    "non_lehman =  pd.concat([filtered_data['db'], filtered_data['gs'], filtered_data['ms'], filtered_data['ubs']])\n",
    "matches = non_lehman[non_lehman.user.isin(matched_users)]\n",
    "\n",
    "matches = matches.groupby('user').first().reset_index()\n",
    "matches['stayed_finance'] = matches['industry'].str.startswith('52', na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "match_to_stayed_finance = dict(zip(matches.user, matches.stayed_finance))\n",
    "match_to_missing_industry = dict(zip(matches.user, pd.isnull(matches.industry)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# match_to_stayed_finance = defaultdict(lambda: None, match_to_stayed_finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lehman_most_recent = lehman.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lehman_most_recent = lehman.groupby('user').first()\n",
    "lehman_most_recent['match_stayed_finance'] = lehman_most_recent.match.apply(lambda x: match_to_stayed_finance[x])\n",
    "lehman_most_recent['lehman_stayed_finance'] = lehman_most_recent.industry.str.startswith('52', na = False)\n",
    "lehman_most_recent['job_category'] = lehman_most_recent.index.to_series().apply(lambda x: full_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "roles = lehman_most_recent.job_category.unique()\n",
    "stayed_finance = {'role' : [], 'lehman_stayed' : [], 'match_stayed' : [], 'total' : [], 'proportion_lehman':[], 'proportion_match' :[], 'zscore': []}\n",
    "\n",
    "for role in roles:\n",
    "    role_data = lehman_most_recent[lehman_most_recent.job_category == role]\n",
    "    lehman_stayed = sum(role_data.industry.str.startswith('52', na = False))\n",
    "    match_stayed = sum(role_data.match_stayed_finance)\n",
    "    total = len(role_data)\n",
    "    prop_lehman = lehman_stayed/total\n",
    "    prop_match = match_stayed/total\n",
    "    zscore = (prop_lehman - prop_match) / (prop_lehman * prop_match *(2/total))**(1/2)\n",
    "    stayed_finance['role'].append(role)\n",
    "    stayed_finance['lehman_stayed'].append(lehman_stayed)\n",
    "    stayed_finance['match_stayed'].append(match_stayed)\n",
    "    stayed_finance['total'].append(total)\n",
    "    stayed_finance['proportion_lehman'].append(prop_lehman)\n",
    "    stayed_finance['proportion_match'].append(prop_match)\n",
    "    stayed_finance['zscore'].append(zscore)\n",
    "role_data = lehman_most_recent[lehman_most_recent.job_category == role]\n",
    "\n",
    "lehman_stayed = sum(lehman_most_recent.industry.str.startswith('52', na = False))\n",
    "match_stayed = sum(lehman_most_recent.match_stayed_finance)\n",
    "total = len(lehman_most_recent)\n",
    "prop_lehman = lehman_stayed/total\n",
    "prop_match = match_stayed/total\n",
    "zscore = (prop_lehman - prop_match) / (prop_lehman * prop_match *(2/total))**(1/2)\n",
    "\n",
    "stayed_finance['role'].append('all_roles')\n",
    "stayed_finance['lehman_stayed'].append(lehman_stayed)\n",
    "stayed_finance['match_stayed'].append(match_stayed)\n",
    "stayed_finance['total'].append(total)\n",
    "stayed_finance['proportion_lehman'].append(lehman_stayed/total)\n",
    "stayed_finance['proportion_match'].append(match_stayed/total)\n",
    "stayed_finance['zscore'].append(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(stayed_finance)\n",
    "\n",
    "df = df[df.total>= 200]\n",
    "\n",
    "df\n",
    "\n",
    "toPlot = df[['role', 'proportion_lehman', 'proportion_match']].set_index('role').stack().reset_index()\n",
    "toPlot = toPlot.rename({'level_1' : 'company', 0 : 'proportion'}, axis = 1)\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,5)})\n",
    "\n",
    "sns.lineplot( x = 'role', y = 'proportion', hue = 'company', data = toPlot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(by = 'role').to_csv('./Deliverables/proportion_finance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing industries Lehman vs match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lehman_most_recent['lehman_missing_industry'] = pd.isnull(lehman_most_recent.industry)\n",
    "lehman_most_recent['match_missing_industry'] = lehman_most_recent.match.apply(lambda x : match_to_missing_industry[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "len(lehman_most_recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(sum(lehman_most_recent.match_missing_industry), sum(lehman_most_recent.lehman_missing_industry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lehman_guys = lehman_most_recent[['lehman_stayed_finance']].copy().rename({'lehman_stayed_finance' : 'stayed_finance'}, axis = 1)\n",
    "\n",
    "lehman_guys['is_lehman'] = 1\n",
    "\n",
    "lehman_guys\n",
    "\n",
    "non_lehman_guys = lehman_most_recent[['match_stayed_finance']].copy().rename({'match_stayed_finance' : 'stayed_finance'}, axis = 1)\n",
    "non_lehman_guys['is_lehman'] = 0\n",
    "\n",
    "all_data = pd.concat([lehman_guys,non_lehman_guys])\n",
    "\n",
    "all_data\n",
    "\n",
    "X = all_data['is_lehman']\n",
    "y = all_data['stayed_finance']\n",
    "\n",
    "\n",
    "model = sm.Probit(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "results.get_margeff().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def prop_breaks(company_name, company_data):\n",
    "    \"\"\"\n",
    "    Outputs proportion of \"TIME_OFF\" or \"MISSING\" entries in a\n",
    "    dataset as the tuple (numerator, denominator).\n",
    "    \"\"\"\n",
    "\n",
    "    def get_breaks():\n",
    "        \"\"\"\n",
    "        Outputs the number of employees that have any \"TIME_OFF\" or \"MISSING\" entries after 2008.\n",
    "        \"\"\"\n",
    "        date_2008 = pd.to_datetime('2008-01-01')\n",
    "        # look only at data after 2008\n",
    "        after_2008 = company_data[company_data['start'] > date_2008]\n",
    "        # groupby user, aggregate by looking at the ticker and seeing if the person has had any time off\n",
    "        return sum(after_2008.groupby('user').ticker.agg(lambda x: any((x == 'TIME_OFF') | (x == 'MISSING'))))\n",
    "    \n",
    "    num_company_users = len(company_data.groupby('user'))\n",
    "    num_breaks = get_breaks()\n",
    "    return company_name, num_breaks, num_company_users, num_breaks/num_company_users\n",
    "\n",
    "drop = ['length', 'gender', 'primary',\n",
    "        'primary_weight', 'secondary', 'secondary_weight',\n",
    "        'city', 'country', 'education', 'elite', '.', '??',\n",
    "        '/', 'department', 'exchange',\n",
    "        'public', 'location_company', 'educational', 'degree', 'elite_education',\n",
    "        'major', 'department', 'FIGI', 'last_update', 'name', 'birth','end', 'role',\n",
    "        'company','normalized_company','industry']\n",
    "\n",
    "breaks_data = {company_name : company_data.drop(labels=drop, axis=1) for company_name, company_data in data.items()}\n",
    "\n",
    "lehman = breaks_data['leh'].copy()\n",
    "date_2008 = pd.to_datetime('2008-01-01')\n",
    "# look only at data after 2008\n",
    "lehman = lehman[lehman['start'] > date_2008]\n",
    "lehman = lehman[lehman.user.isin(list(lehman_to_match.keys()))]\n",
    "\n",
    "lehman['match'] = lehman.user.apply(lambda x : lehman_to_match[x])\n",
    "prop_breaks('leh', lehman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "matches = list(lehman.match.unique())\n",
    "\n",
    "non_lehman =  pd.concat([breaks_data['db'], breaks_data['gs'], breaks_data['ms'], breaks_data['ubs']])\n",
    "\n",
    "non_lehman_matches = non_lehman[non_lehman.user.isin(matches)]\n",
    "\n",
    "date_2008 = pd.to_datetime('2008-01-01')\n",
    "# look only at data after 2008\n",
    "after_2008 = non_lehman_matches[non_lehman_matches['start'] > date_2008]\n",
    "# groupby user, aggregate by looking at the ticker and seeing if the person has had any time off\n",
    "y = after_2008.groupby('user').ticker.agg(lambda x: any((x == 'TIME_OFF') | (x == 'MISSING')))\n",
    "\n",
    "w = y.to_frame()\n",
    "\n",
    "r = w[w.ticker == True]\n",
    "\n",
    "r = list(r.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "non_lehman_matches['took_break'] = False\n",
    "\n",
    "non_lehman_matches.loc[non_lehman_matches.user.isin(r), 'took_break'] = True\n",
    "\n",
    "user_to_break = dict(zip(non_lehman_matches.user, non_lehman_matches.took_break))\n",
    "\n",
    "x = lehman.groupby('user').first()\n",
    "\n",
    "x['break'] = x.match.apply(lambda x: user_to_break[x])\n",
    "\n",
    "('other', sum(x['break']), len(x), sum(x['break'])/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lehman_guys = lehman.groupby('user').first()\n",
    "lehman_guys['break'] = lehman.groupby('user').ticker.agg(lambda x: any((x == 'TIME_OFF') | (x == 'MISSING'))).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lehman_guys['is_lehman'] = 1\n",
    "lehman_guys = lehman_guys[['break', 'is_lehman']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "non_lehman_guys = x[['break']].copy()\n",
    "non_lehman_guys['is_lehman'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([lehman_guys,non_lehman_guys])\n",
    "\n",
    "all_data\n",
    "\n",
    "X = all_data['is_lehman']\n",
    "y = all_data['break']\n",
    "\n",
    "\n",
    "model = sm.Probit(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "results.get_margeff().summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
